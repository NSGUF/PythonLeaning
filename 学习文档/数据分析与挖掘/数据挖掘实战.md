# 第1章 数据挖掘基础
>**概念：**从大量数据中挖掘出隐含的、未知的、对决策有潜在价值的关系、模式、趋势，并用这些知识和规则建立用于决策支持的模型，提供预测性决策支持的方法、工具和过程，就是**数据挖掘**；是利用各种分析工具在大量数据中寻找其规律和发现模型与数据之间关系的过程，是统计学、数据库技术和人工智能技术的综合。  
>**基本任务：**利用分类与预测、聚类分析、关联规则、时序模式、偏差监测、智能推荐等方法，帮助企业提取数据中蕴含的商业价值，提高企业竞争能力。  
>**数据挖掘建模过程：**  
>1、定义挖掘目标；  
>2、数据取样，常用有：随机、等距、分层、从起始顺序、分类；  
>3、数据探索，主要有：异常值分析、缺失值分析、相关分析和周期性分析等。  
>4、数据预处理：主要有：数据筛序、数据变量转换、缺失值处理、坏数据处理、数据标准化、主成分分析、属性选择、数据违约等；  
>5、挖掘建模；  
>6、模型评价；
# 第2章 Python数据分析简介  
* [python学习笔记](http://www.cnblogs.com/NSGUF/p/7459427.html)  
* [Numpy学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Scipy学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Matplotlib学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Pandas学习笔记](http://www.cnblogs.com/NSGUF/p/8127673.html)  
* [StatsModels学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* Scikit-Learn  
	>该库包括数据预处理、分类、回归、聚类、预测和模型分析等。使用方法如下：  
	  
		# 1、导入模型
		from sklearn.linear_model import LinearRegression
		# 创建对象
		model=LinearRegression()
		# 所有模型提供的接口有
		model.fit()# 训练模型
		# 监督模型提供的接口有
		model.predict(X_new)# 预测新样本
		model.predict_proba(X_new)# 预测概率，仅对某些模型有用如：LR
		model.score()# 得分越高，fit越好
		# 非监督模型提供的接口有
		model.transforms()# 从数据中学到新的“基空间”
		model.transforms()# 从数据中学到新的“基空间”并将这个数据按照数组‘基’进行转换
* [Keras学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
# 第3章 数据探索
>**概念：**通过检验数据集的数据质量、绘制图标、计算某些特征向量等手段，对样本数据集的结构和规律进行分析的过程。
## 3.1 数据质量分析
>该过程是数据预处理的前提，也是数据挖掘分析结论有效性和准确性的基础。  
>数据质量分析的主要目的是检查原始数据中是否存在脏数据，脏数据包括以下：    
>
* 缺失值  
* 异常值
* 不一致的值
* 重复数据及含有特殊符号（如#、￥、*）等数据  

### 3.1.1 缺失值分析
1、产生原因：  
    1）某些信息无法被获取或获取信息代价太大。  
    2）被遗漏。  
    3）属性值不存在。  
2、缺失值的影响：  
1）丢失大量的有用信息。  
2）模型中所蕴含的规律更难把握。  
3）导致不可靠输出。  
3、缺失值的分析：  
统计含有缺失值属性的个数以及每个属性的未缺失数、缺失数与缺失率。  
4、缺失值处理：  
1）删除存在的缺失值。  
2）对可能进行插补。  
3）不处理。
### 3.1.2 异常值分析  
>异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也成为**离群值**，异常值分析也称为离群点分析。  
1.简单统计量分析：变量取值是否合理，如年龄不超过200。  
2.3σ原则：若数据服从正态分布，在3σ原则下，异常值为测定值与平均值的偏差超过3倍标准差的值。若不服从，则用远离平均值的多少倍标准差来描述。  
3.箱型图分析：异常值被定义为小于Q<sub>l</sub>-1.5IQR或大于Q<sub>u</sub>+1.5IQR的值。Q<sub>l</sub>称为下四分位数，表示全部观察值中有四分之一的数据取值比它小；Q<sub>u</sub>称为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR称为四分位数间距是Q<sub>l</sub>与Q<sub>u</sub>的差值，期间包含了全部观察值的一半。箱型图识别异常值的结果比较客观，在识别异常值方面有一定的优越性。下面给出一个例子：

	# 异常值检查
	import pandas as pd
	import matplotlib.pyplot as plt
	catering_sale='catering_sale.xls' # 餐饮数据
	data=pd.read_excel(catering_sale,index_col='日期')# 读取数据并指定列为日期
	print(data.describe())# 打印出数据的基本数据，count表示非空值数
	print(len(data))# 总共条数
	
	plt.figure()
	p=data.boxplot(return_type='dict')# 画箱型图
	x=p['fliers'][0].get_xdata()
	y=p['fliers'][0].get_ydata()
	y.sort()
	for i in range(len(x)):
	    if i>0:
	        plt.annotate(y[i],xy=(x[i],y[i]),xytext=(x[i]+0.05-0.8/(y[i]-y[i-1]),y[i]))
	    else:
	        plt.annotate(y[i], xy=(x[i], y[i]), xytext=(x[i] + 0.8, y[i]))
	plt.show()

### 3.1.3 一致性分析
>数据不一致性是指数据的矛盾性和不相容性。产生原因主要发生在数据集成的过程中，这可能是由于被挖掘数据是来自于从不同的数据源、对于重复存放的数据未能进行一致性更新。例如两张表中都存储了用户的电话，但在用户的电话发生改变时，只更新了一张表的数据，那么这两张表中就有了不一致的数据。
## 3.2数据特征分析
### 3.2.1 分布分析  
>分布分析可以揭示数据的分布特征和分布类型。对于定量数据，想要了解其分布形式是对称还是非对称，发现某些特大特效的可疑值，可通过绘制频率分布表和频率分布直方图、茎叶图进行直观观察；对于定性分类数据，可用饼状图和条形图直观地显示分布情况。  

**1、定量数据的分布分析**  
1）求极差：最大值-最小值  
2）决定组距和组数：组数=极差/组距  
3）决定分点：分布区间  
4）绘制频率分布表  
5）绘制频率分布直方图  
**2、定性数据的分布分析**  
根据变量的分类类型来分组，可采用饼图和条形图。
### 3.2.2 对比分析
>对比分析是把两个相互关联的指标进行比较，从数量上展示和说明研究对象规模的大小、水平的高低、速度的快慢，以及各种关系是否协调。特别适用于指标间的横纵向比较、时间序列的比较分析。对比分析主要有两种形式：
（1）绝对数比较：绝对数比较是利用绝对数进行比较。
（2）相对数比较：由两个有联系的指标对比计算，用以客观现象之间数量联系程度的综合指标，其数值表现为相对数。由于研究目的和对比基础不同，相对数可以分为以下几种：  
1）结构相对数：将同一总体内的部分数值与全部数值对比求得比重，用以说明事物的性质、结构或质量。如合格率  
2）比例相对数：将同一总体内不同部分的数值进行对比，表明总体内各部分的比例关系。如男女比例  
3）比较相对数：将同一时期两个性质相同的指标数值进行对比，说明同类现象在不同空间下的数量对比关系。如不同地区上品价格比  
4）强度相对数：将两个性质不同但有一定联系的总量指标进行对比，用以说明现象强度、密度和普遍程度。如人均国内生产总值  
5）计划完成程度相对数：某一时期实际完成数与计划书的对比，用以说明计划完成程度。  
6）动态相对数：将同一现象在不同时期的指标数值进行对比，用以说明发展方向和变化速度。如增长速度  
### 3.2.3 统计量分析  
>用统计指标对定量数据进行统计描述。  
**1、集中趋势度量**  
1）均值，若不同成分所占的不同重要程度，可为数据集的每个值都添加一个权重；若数据中存在极端值或数据是偏态分布，那么均值便不能很好地度量数据的集中趋势，为了消除少数极端值的影响，可以使用截断均值或中位数来度量数据的及中国趋势。截断均值是去除高、低极端值之后的平均数。  
2）中位数：数据集中的值从小到大排序，位于中间的数。当数据个数为偶数，则是中间两个数的平均值。  
3）众数：指数据集中出现最频繁的值；众数不经常用来度量定性变量的中心位置，更适合用于定性变量。众数不具有唯一性，一般用于离散型变量而非连续性变量。  
**2、离中趋势度量**  
1）极差  
2）标准差：度量数据偏离均值的程度  
3）异变系数：度量标准差相对于均值的离中趋势；值为（标准差/平均值），主要用来比较两个或多个具有不同单位或不同波动幅度的数据集的离中趋势。  
4）四分位数间距：将所有数值从小到大排列分为四等份，处于第一个分割点的是下四分位，第二个分割点位位于中位数的位置，第三个分割点位置的数值是上四分位数。四分位间距是上四分位和下四分位的差值，其值越大，说明数据的变异程度越大，反之，越小。  
餐饮数据统计量分析例子如下：  

	import pandas as pd
	catering_scale='catering_sale.xls'
	data=pd.read_excel(catering_scale,index_col='日期')
	data=data[(data['销量']>400)&(data['销量']<5000)]# 过滤异常值
	
	statistics=data.describe()# 获取结果
	
	statistics.loc['range']=statistics.loc['max']-statistics.loc['min']# 添加极值
	statistics.loc['var']=statistics.loc['std']-statistics.loc['mean']# 添加异变系数
	statistics.loc['dis']=statistics.loc['75%']-statistics.loc['25%']# 四分位间距
	print(statistics)

### 3.2.4 周期性分析  
>探索某个变量是否随时间变化而呈现出某种周期变化趋势。时间尺度相对较长的周期性趋势有年度周期性趋势、季节性周期趋势，相对较短的有月度周期性趋势、周度周期性趋势，甚至更短的天、小时周期性趋势。
### 3.2.5 贡献度分析  
>贡献度分析又称帕累托分析，原理来自帕累托法则，又称20/80定律。重点改善盈利最高的前80%，可增加盈利。给出画帕累托的例子：  

	import pandas as pd
	import matplotlib.pyplot as plt
	
	catering_dish = 'catering_dish_profit.xls'
	data = pd.read_excel(catering_dish, index_col='菜品名')  # 读取信息
	print(data)
	data = data['盈利'].copy()
	
	data = data.sort_values(ascending=False)  # 对盈利倒序
	
	plt.figure()
	data.plot(kind='bar')
	p = 1.0 * data.cumsum() / data.sum()# 比例
	p.plot(color='r', secondary_y=True, style='-o', linewidth=2)
	plt.annotate(format(p[6], '.4%'), xy=(6, p[6]), xytext=(6 * 0.9, p[6] * 0.9),
	             arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))  # 添加注释，即85%处的标记。这里包括了指定箭头样式。
	plt.show()

### 3.2.6 相关性分析 
>分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析。  
**1.直接绘制散点图**    
判断两个变量是否具有线性相关关系的最直观方法是直接绘制散点图。  
**2.绘制散点图矩阵**  
若需要同时考察多个变量间的相关关系时，一一绘制他们间的简单散点图是十分麻烦的。此时可利用散点图矩阵同时绘制各变量间的散点图，从而快速发现多个变量间的主要相关性。在多元线性回归有重要作用。  
**3.计算相关系数**    
为了更加准确地描述变量之间的线性相关程度，可通过计算相关系数进行相关分析。  
1）Pearson相关系数，一般用于分析两个连续性变量之间的关系，要求服从正态分布：  
![](http://img.my.csdn.net/uploads/201211/21/1353507674_8005.png)   
相关系数r的取值范围：-1<=r<=1;  
0<|r|<1表示存在不同程度线性相关：  
若0.3<|r|<=0.5 低度线性相关    
若0.5<|r|<=0.8 显著线性关系  
若|r|>0.8 高度线性关系  
2）Spearman秩相关系数，不服从正态分布的变量、分类或等级变量之间的关联性。   
只要两个变量具有严格单调的函数关系，那么他们就是完全Spearman相关，这与Pearson相关不同，Pearson相关只有在变量具有线性关系时才完全相关。研究表明，在正态分布假定的情况下，Spearman秩相关系数与Pearson相关系数在效率上等价，而对于连续测量数据，更适合用Pearson相关系数来进行分析。  
3）判定系数  
判定系数是相关系数的平方，用r<sup>2</sup> 表示；用来衡量回归方程对y的解释程度。判定系数取值范围：0<=r<sup>2</sup><=1。r<sup>2</sup>越接近1，表明x与y之间的相关性越强；反之月接近0，表明两个变量之间几乎没有直线相关关系。  

	# 不同菜品之间的关系
	import pandas as pd
	
	data=pd.read_excel('catering_sale_all.xls',index_col='日期') 
	print(data.corr())# 相关系数矩阵，即给出了任意两个菜之间的关系  Spearman(Pearman)
	print(data.corr()['百合酱蒸凤爪']) # 给出了百合酱蒸凤爪与其他任意菜之间的关系

## 3.3 Python主要数据探索函数  
* [python学习笔记](http://www.cnblogs.com/NSGUF/p/7459427.html) 
* [Matplotlib学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
# 第4章 数据预处理  
>数据预处理的主要内容包括数据清洗、数据集成、数据变换和数据规约。在数据挖掘的过程中，数据预处理工作量占整个过程的60%：  
## 4.1 数据清洗  
>删除原始数据集中无关数据、重复数据、平滑噪声数据，筛选掉与挖掘主题无关的数据，处理缺失值、异常值等。  
### 4.1.1 缺失值处理  
方法有：删除记录、数据插补、不处理。其中数据插补常用方法有：均值/中位数/众数插补、使用固定值、最近临插补、回归方法、插值法。  
1）拉格朗日插值法  
![](http://upload.wikimedia.org/math/d/6/2/d62a2ffab8cf2363fcc1f26f388838d3.png)  
该方法结构紧凑，在理论分析中方便，但是当差值节点增减时，差值多项式就会随之变化，这在实际计算中十分不便。  
例子：  

	import pandas as pd
	from scipy.interpolate import lagrange
	
	inputfile = 'catering_sale.xls'
	outputfile = 'sales.xls'
	
	data = pd.read_excel(inputfile)  # 读取excel
	data.loc[(data['销量'] < 400) | (data['销量'] > 5000), '销量'] = None  # 异常值变为空值
	
	def ployinterp_column(s, n, k=5):  # 默认是前后5个
	    y = s[list(range(n - k, n)) + list(range(n + 1, n + 1 + k))]  # 取数，n的前后5个，这里有可能取到不存在的下标，为空
	    y = y[y.notnull()]  # 如果y里面有空值的话就去掉
	    return lagrange(y.index, list(y))(n)  # 最后的括号就是我们要插值的n
	
	for i in data.columns:
	    if i == '日期':
	        continue
	    for j in range(len(data)):
	        if (data[i].isnull())[j]:  # 空值进行插值
	            data.loc[j, i] = ployinterp_column(data[i], j)
	data.to_excel(outputfile)  

### 4.1.2 异常值处理  
* 删除含有异常值的记录  
* 视为缺失值再处理（利用现有数据对异常值进行填补）  
* 平均值修正，前后两个观察值得平均 
* 不处理
## 4.2 数据集成  
>数据挖掘需要的数据往往分布在不同的数据源中，数据集成就是将多个数据源合并存放在一个一致的数据存储中的过程。  
>在数据集成时，来自多个数据源的现实世界实体的表达形式不一样，有可能不匹配，要考虑实体识别问题和属性冗余问题，从而将数据源在最底层加以转换、提炼和集成。  
### 4.2.1 实体识别  
>实体识别是指从不同数据源识别现实世界的实体，它的任务是统一不同数据源的矛盾之处，常见形式如下：  

* 同名异义
* 异名同义
* 单位不统一   
### 4.2.2 冗余属性识别  
>仔细整合不同源数据能减少甚至避免数据冗余与不一致，从而提高数据挖掘的速度和质量。有些冗余属性可用相关分析检测，给定两个数值型的属性A和B，根据其属性值，用相关系数度量一个属性在多大程度上蕴含另一个属性，可见3.2.6。冗余形式如下：  

* 同一属性多次出现  
* 同一属性命名不一致导致重复

## 4.3 数据变换  
>对数据进行规范化处理，将数据转换成“适当的”形式，以适用于挖掘任务及算法的需要。
### 4.3.1 简单函数变换  
>简单函数变换是对原始数据进行某些数学函数变换，常用的变换包括平方、开方、取对数、差分运算等。常用来将不具有正态分布的数据变换成具有正态分布的数据。在时间序列分析中，有时简单的对数变换或者差分运算就可以将非平稳序列转换成平稳序列。在数据挖掘中，简单的函数变换可能更有必要，如个人年收入取值范围为10000到10亿，区间较大，可使用对数变换进行压缩。  
### 4.3.2 规范化  
>不同评价指标往往具有不同的量纲，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果。为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，使之落入一个特定的区域，便于进行综合分析。对于基于距离的挖掘算法尤为重要。方法有以下几种：  

* 最小-最大规范化  
>也称为离差标准化，对原始数据的线性变换，将数值映射得到[0,1]之间，公式为：x<sup>*</sup>=(x-min)/(max-min)；该方法保留了原来数据中存在的关系，消除量纲和数据取值范围影响的最简单方法。这种处理方法的缺点是若数值集中且数值很大，则规范化后各值会接近于0，并且将会相差不大，若将来遇到超过目前属性[min,max]取值范围的时候，会引起系统出错，需要重新确定min和max。 
 
* 零-均值规范化  
>也称为标准差标准化，经过处理的数据的均值为0，标准差为1。转换公式为：x<sup>*</sup>=(x-原始数据均值）/原始数据的标准差，这种方法是目前用的最多的数据标准化方法。    

* 小数定标规范化  
>通过移动属性值的小数位数，将属性值映射到[-1,1]之间，移动的小数位数取决于属性值绝对值的最大值。转换公式：x<sup>*</sup>=x/10<sup>k</sup>  

	import pandas as pd
	import numpy as np
	datafile='normalization_data.xls'
	data=pd.read_excel(datafile,header=None)
	# 最小-最大规范化
	print((data-data.min())/(data.max()-data.min()))
	# 零-均值规范化
	print((data-data.mean())/data.std())
	# 小数定标规范化
	print(data/10**np.ceil(np.log10(data.abs().max())))
### 4.3.3 连续属性离散化  
>一些数据挖掘算法，特别是某些分类算法（如ID3算法、Apriori算法等）要求数据是分类属性形式，这样常常需要将连续属性表换成分类属性，即连续属性离散化    

1.离散化的过程  
>连续属性的离散化是在数据的取值范围内设定若干个离散的划分点，将取值范围划分为一些离散化的区间，最后用不同的符号或整数值代表落在每个字区间中的数据值。所以，离散化涉及两个子任务：确定分类数以及如何将连续属性值映射到这些分类值。  

2.常用的离散化方法   

* 等宽法
>将属性的值域分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者由用户指定，类似于制作频率分布表。  

* 等频法  
>将相同数量的记录放进每个区间。  

* 基于聚类分析的方法  
>首先将连续属性的值用聚类算法（如K-Means算法）进行聚类，然后再将聚类得到的簇进行处理，合并到一个簇的连续属性值并做同一标记。聚类分析的离散化方法也需要用户指定簇的个数。  
> 等宽法和等频法方法简单，易操作，但都需要人为的规划好区间个数；等宽法缺点在于它对力群点比较敏感，倾向于不均匀地把属性值分布到各个区间。有些区间包含许多数据，有些区间数据极少。等频虽然避免了该问题，但可能将相同的数据值分到不同的区间以满足每个区间中固定的数据个数。  
### 4.3.4 属性构造  
>为了提取更有用的信息，挖掘更深层次的模式，提高挖掘结果的精度，我们需要利用已有的属性集构建新的属性，并加入到现有的属性中。  

	import pandas as pd
	inputfile='electricity_data.xls'
	outfile='electricity_data_out.xls'
	data=pd.read_excel(inputfile)
	data['线损率']=(data['供入电量']-data['供出电量'])/data['供入电量']
	data.to_excel(outfile,index=False)# index表示行号是否显示

### 4.3.5 小波变换  
>小波变换具有多分辨率的特点，在时域和频域都具有表征信号局部特征的能力，通过伸缩和平移等运算过程对信号进行多尺度聚焦分析，提供了一种非平稳信号的时频分析手段，可以由粗及细地逐步观察信号，从中提取有用信息。能刻画某个问题的特征量往往是隐含在一个信号中的某个或者某个分量中，小波变换可以把非平稳信号分解为表达层次不同、不同频带信息的数据序列，即小波系数。选取适当的小波系数，即完成了信号的特征提取。  

1、基于小波变换的特征提取方法    

* 基于小波变换的多尺度空间能量分布特征提取方法  
>各尺度空间内的平滑信号和细节信号能提供原始信号的时频局域信息，特别是能提供不同频段上信号的不同信息。把不同分解尺度上的信息能量求解出来，就可将这些能量尺度顺序排列，形成特征向量供识别用。

* 基于小波变换的多尺度空间的模极大值特征提取方法  
>利用小波变换的信号局域化分析能力，求解小波变化的模极大值特征来检测信号的局部奇异性，将小波变换模极大值的尺度参数s、平移参数t及其幅值作为目标的特征量。

* 基于小波包变换的特征提取
>利用小波分解，可将时域随机信号序列映射为尺度预各子空间内的随机系数序列，按小波包分解得到的最佳子空间内随机系数序列的不确定性程度最低，将最佳子空间的熵值即最佳子空间在完整二叉树中的位置参数作为特征量，可用于目标识别。

* 基于适应性小波神经网络的特征提取方法
>基于适应性小波神经网络的特征提取方法可以把信号通过分析小波拟合表示，进行特征提取。

2、小波基函数  
>小波基函数是一种具有局部支集的函数，并且平均值为0。常用有：Haar小波基，db系列小波基等。

3、小波变换
>对小波基函数进行伸缩和平移变换：

4、基于小波变换的多尺度空间能量分布特征提取方法  
>利用小波变换可以对声波信号进行特征特区，提取出可以代表声波信号的向量数据，即完成从声波信号到特征向量数据的变换。  
	inputfile='leleccum.mat'
	
	from scipy.io import loadmat
	mat=loadmat(inputfile)# mat位python专属格式，需要用loadmat读取
	signal=mat['leleccum'][0]
	import pywt
	coeffs=pywt.wavedec(signal,'bior3.7',level=5)# 小波变换特征提取，返回level+1个数字，第一个数组为逼近系数数组，后面的依次为细节系数数组
	print(coeffs)

## 4.4 数据规约
>数据规约产生更小但担保原数据完整性的新数据集。其意义在于：

* 降低无效、错误数据对建模的影响，提高建模的准确性；
* 少量且具代表性的数据将大幅缩减数据挖掘所需要的时间；
* 降低存储数据的成本。

### 4.4.1 属性规约
>属性规约通过属性合并来创建新属性维数，或者直接通过删除不相关的属性（维）来减少数据维数，从而提高数据挖掘的效率、降低计算成本。属性规约的目标是寻找出最小的属性子集并确保新数据子集的概率分布尽可能地接近原来数据集的概率分布。  
常用方法：  

* 合并属性
>将一些旧属性合为新属性  

* 逐步向前选择  
>从一个空集开始，每次从原来的属性集合中选择一个当前最优的属性并添加到当前属性子集中，直到无法选择出最优属性或满足一定阙值约束为止。  

* 逐步向后选择
>从一个全属性集开始，每次从当前属性集合中选择一个当前最差的属性并将其从当前属性子集中剔除，直到无法选择出最差属性或满足一定阙值约束为止。

* 决策树归纳  
>利用决策树的归纳方法对初始数据进行分类归纳学习，获得初始决策树，所有没有出现在决策树上的属性均可认为是无关属性，因此将这个属性从初始集合中删除即可获得较优属性子集。

* 主成分分析
>用较少的变量去解释原数据中的大部分变量，即将许多相关性很高的变量转换成彼此相互独立或不相关的变量。

>逐渐向前、向后、决策树属于直接删除不想关属性方法。主成分分析是一种用于连续属性的数据降维方法，它构造了原始数据的一个正交变换，新空间的基底去除了原始空间基底下数据的相关性，只需使用少数新变量就能解释原始数据中的大部分变量。

	import pandas as pd
	from sklearn.decomposition import PCA
	inputfile = 'principal_component.xls'
	outputfile = 'dimention_reducted.xls'
	data=pd.read_excel(inputfile,header=None)
	
	pca=PCA(n_components=None,copy=True,whiten=False)# n_components表示算法中保留的主成分个数，int或string，即保留下来的特征个数，缺省时默认为None，所有成分被保留，为string时，如n_components=‘mle'表示自动选择特征个数n，使得满足所有要求的方差百分比，copy，默认为True，表示是否在运行算法时，将原始训练数据复制一份，若为True，则运行PCA算法后，原始数据不会有任何改变，因为是在副本上运行，若为False，则运行PCA算法后，原始数据会改变，即在原始数据中进行降维。whiten表是否白化，使得每个特征具有相同的方差。
	
	pca.fit(data)
	print(pca.components_)# 返回模型的各个特征向量
	print(pca.explained_variance_ratio_)# 返回各个成分各自的方差百分比
	
	pca=PCA(3)
	pca.fit(data)
	low_d=pca.transform(data)# 降低维度
	pd.DataFrame(low_d).to_excel(outputfile) # 保存结果
	pca.inverse_transform(low_d)# 复原数据
### 4.4.2 数值规约
>数值规约指通过选择代替的、较小的数据来减少数据量，包括有参数方法和无参数方法两类。有参数方法是使用一个模型来评估数据，只需存放参数，而不需要存放实际数据，例如回归（线性回归和多元回归）和对数线性模型（近似离散属性集中的多维概率分布）。无参数方法就需要存放实际数据，例如直方图、聚类、抽样。

* 直方图
* 聚类
* 抽样
* 参数回归

## 4.5 Python主要数据预处理函数
* interpolate
>一维、高维数据插值，包含了大量的插值函数如拉格朗日插值、样条插值、高维插值等；使用：from scipy.interpolate import *引入。

* unique
>去除数据的重复元素，得到单值元素列表；使用格式：np.unique(一维数据)或Series.unique()

* isnull/notnull
>判断元素是否空值/非空值；使用格式：Series.isnull()/Series.notnull()，可通过Series[Series.isnul()]找出空值。

* random
>生成随机矩阵；使用格式：np.random.rand(k,m,n),k*m*n的随机均匀分布在(0,1)上，np.random.randn(k,m,n)，k*m*n随机标准正态分布；

* PCA
>对指标变量矩阵进行主成分分析，可看4.4.1小节。
# 第5章 挖掘建模
## 5.1 分类与预测
>分类和预测是预测问题的两种主要类型，分类主要是预测分类标号（离散属性），而预测主要是建立连续值函数模型，预测给定自变量对应的因变量的值。

### 5.1.1 实现过程
>1）分类：分类是构造一个分类模型，输入样本的属性值，输出对应的类别，将每个样本映射到预先定义好的类别。分类模型定义在已有类标记的数据集上，模型在已有样本上的准确率可以方便的计算，所以分类属于监督的学习。  
>2）预测：指建立两种或两种以上变量间互相依赖的函数模型，然后进行预测或控制。  

### 5.1.2 常用的分类与预测算法
* 回归分析：确定预测属性（数值型）与其他变量间相互依赖的定量关系最常用统计学方法。
* 决策树：采用自顶向下的递归方法，在内部节点进行属性值得比较，并根据不同的属性值从该节点向下分支，最终得到的叶节点是学习划分的类。
* 人工神经网络：模仿大脑神经网络结构和功能而建立的信息处理系统，表示神经网络的输入与输出变量之间关系的模型。
* 贝叶斯网络：Bayes方法的扩展。
* 支持向量机：一种通过某种非线性映射，把低维的非线性可分转化为高维的线性可分，在高维空间进行线性分析的算法。

### 5.1.3 回归分析
>回归分析是通过建立模型来研究变量之间相互关系的密切程度、结构状态及进行模型预测的一种有效供给，在工商管理、经济、社会、医学和生物学等领域应用十分广泛。主要回归模型分类如下：  

* 线性回归
>因变量与自变量是线性关系，可用最小二乘法求解模型系数。

* 非线性模型
>因变量与自变量之间不都是线性关系，若非线性关系可通过简单的函数变换转化成线性关系，用线性回归的思想求解，若不能转换，则可用非线性最小二乘方法求解。

* Logistic回归
>因变量一般有1和0（是否）两种取值，在广义线性回归模型的特例，利用Logistic函数将因变量的取值范围控制在0和1之间，表示取值为1的概率。属于概率型非线性回归。

* 岭回归
>参与建模的自变量之间具有多重共线性，是一种改进最小二乘估计的方法。

* 主成分回归
>参与建模的自变量之间具有多重共线性，是对最小二乘法的一种改进，它是参数估计的一种有偏估计，可以消除自变量之间的多重共线性。