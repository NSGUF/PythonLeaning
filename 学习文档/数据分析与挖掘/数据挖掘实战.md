# 第1章 数据挖掘基础
>**概念：**从大量数据中挖掘出隐含的、未知的、对决策有潜在价值的关系、模式、趋势，并用这些知识和规则建立用于决策支持的模型，提供预测性决策支持的方法、工具和过程，就是**数据挖掘**；是利用各种分析工具在大量数据中寻找其规律和发现模型与数据之间关系的过程，是统计学、数据库技术和人工智能技术的综合。  
>**基本任务：**利用分类与预测、聚类分析、关联规则、时序模式、偏差监测、智能推荐等方法，帮助企业提取数据中蕴含的商业价值，提高企业竞争能力。  
>**数据挖掘建模过程：**  
>1、定义挖掘目标；  
>2、数据取样，常用有：随机、等距、分层、从起始顺序、分类；  
>3、数据探索，主要有：异常值分析、缺失值分析、相关分析和周期性分析等。  
>4、数据预处理：主要有：数据筛序、数据变量转换、缺失值处理、坏数据处理、数据标准化、主成分分析、属性选择、数据违约等；  
>5、挖掘建模；  
>6、模型评价；
# 第2章 Python数据分析简介  
* [python学习笔记](http://www.cnblogs.com/NSGUF/p/7459427.html)  
* [Numpy学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Scipy学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Matplotlib学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* [Pandas学习笔记](http://www.cnblogs.com/NSGUF/p/8127673.html)  
* [StatsModels学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
* Scikit-Learn  
	>该库包括数据预处理、分类、回归、聚类、预测和模型分析等。使用方法如下：  
	  
		# 1、导入模型
		from sklearn.linear_model import LinearRegression
		# 创建对象
		model=LinearRegression()
		# 所有模型提供的接口有
		model.fit()# 训练模型
		# 监督模型提供的接口有
		model.predict(X_new)# 预测新样本
		model.predict_proba(X_new)# 预测概率，仅对某些模型有用如：LR
		model.score()# 得分越高，fit越好
		# 非监督模型提供的接口有
		model.transforms()# 从数据中学到新的“基空间”
		model.transforms()# 从数据中学到新的“基空间”并将这个数据按照数组‘基’进行转换
* [Keras学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
# 第3章 数据探索
>**概念：**通过检验数据集的数据质量、绘制图标、计算某些特征向量等手段，对样本数据集的结构和规律进行分析的过程。
## 3.1 数据质量分析
>该过程是数据预处理的前提，也是数据挖掘分析结论有效性和准确性的基础。  
>数据质量分析的主要目的是检查原始数据中是否存在脏数据，脏数据包括以下：    
>
* 缺失值  
* 异常值
* 不一致的值
* 重复数据及含有特殊符号（如#、￥、*）等数据  

### 3.1.1 缺失值分析
1、产生原因：  
    1）某些信息无法被获取或获取信息代价太大。  
    2）被遗漏。  
    3）属性值不存在。  
2、缺失值的影响：  
1）丢失大量的有用信息。  
2）模型中所蕴含的规律更难把握。  
3）导致不可靠输出。  
3、缺失值的分析：  
统计含有缺失值属性的个数以及每个属性的未缺失数、缺失数与缺失率。  
4、缺失值处理：  
1）删除存在的缺失值。  
2）对可能进行插补。  
3）不处理。
### 3.1.2 异常值分析  
>异常值是指样本中的个别值，其数值明显偏离其余的观测值。异常值也成为**离群值**，异常值分析也称为离群点分析。  
1.简单统计量分析：变量取值是否合理，如年龄不超过200。  
2.3σ原则：若数据服从正态分布，在3σ原则下，异常值为测定值与平均值的偏差超过3倍标准差的值。若不服从，则用远离平均值的多少倍标准差来描述。  
3.箱型图分析：异常值被定义为小于Q<sub>l</sub>-1.5IQR或大于Q<sub>u</sub>+1.5IQR的值。Q<sub>l</sub>称为下四分位数，表示全部观察值中有四分之一的数据取值比它小；Q<sub>u</sub>称为上四分位数，表示全部观察值中有四分之一的数据取值比它大；IQR称为四分位数间距是Q<sub>l</sub>与Q<sub>u</sub>的差值，期间包含了全部观察值的一半。箱型图识别异常值的结果比较客观，在识别异常值方面有一定的优越性。下面给出一个例子：

	# 异常值检查
	import pandas as pd
	import matplotlib.pyplot as plt
	catering_sale='catering_sale.xls' # 餐饮数据
	data=pd.read_excel(catering_sale,index_col='日期')# 读取数据并指定列为日期
	print(data.describe())# 打印出数据的基本数据，count表示非空值数
	print(len(data))# 总共条数
	
	plt.figure()
	p=data.boxplot(return_type='dict')# 画箱型图
	x=p['fliers'][0].get_xdata()
	y=p['fliers'][0].get_ydata()
	y.sort()
	for i in range(len(x)):
	    if i>0:
	        plt.annotate(y[i],xy=(x[i],y[i]),xytext=(x[i]+0.05-0.8/(y[i]-y[i-1]),y[i]))
	    else:
	        plt.annotate(y[i], xy=(x[i], y[i]), xytext=(x[i] + 0.8, y[i]))
	plt.show()

### 3.1.3 一致性分析
>数据不一致性是指数据的矛盾性和不相容性。产生原因主要发生在数据集成的过程中，这可能是由于被挖掘数据是来自于从不同的数据源、对于重复存放的数据未能进行一致性更新。例如两张表中都存储了用户的电话，但在用户的电话发生改变时，只更新了一张表的数据，那么这两张表中就有了不一致的数据。
## 3.2数据特征分析
### 3.2.1 分布分析  
>分布分析可以揭示数据的分布特征和分布类型。对于定量数据，想要了解其分布形式是对称还是非对称，发现某些特大特效的可疑值，可通过绘制频率分布表和频率分布直方图、茎叶图进行直观观察；对于定性分类数据，可用饼状图和条形图直观地显示分布情况。  

**1、定量数据的分布分析**  
1）求极差：最大值-最小值  
2）决定组距和组数：组数=极差/组距  
3）决定分点：分布区间  
4）绘制频率分布表  
5）绘制频率分布直方图  
**2、定性数据的分布分析**  
根据变量的分类类型来分组，可采用饼图和条形图。
### 3.2.2 对比分析
>对比分析是把两个相互关联的指标进行比较，从数量上展示和说明研究对象规模的大小、水平的高低、速度的快慢，以及各种关系是否协调。特别适用于指标间的横纵向比较、时间序列的比较分析。对比分析主要有两种形式：
（1）绝对数比较：绝对数比较是利用绝对数进行比较。
（2）相对数比较：由两个有联系的指标对比计算，用以客观现象之间数量联系程度的综合指标，其数值表现为相对数。由于研究目的和对比基础不同，相对数可以分为以下几种：  
1）结构相对数：将同一总体内的部分数值与全部数值对比求得比重，用以说明事物的性质、结构或质量。如合格率  
2）比例相对数：将同一总体内不同部分的数值进行对比，表明总体内各部分的比例关系。如男女比例  
3）比较相对数：将同一时期两个性质相同的指标数值进行对比，说明同类现象在不同空间下的数量对比关系。如不同地区上品价格比  
4）强度相对数：将两个性质不同但有一定联系的总量指标进行对比，用以说明现象强度、密度和普遍程度。如人均国内生产总值  
5）计划完成程度相对数：某一时期实际完成数与计划书的对比，用以说明计划完成程度。  
6）动态相对数：将同一现象在不同时期的指标数值进行对比，用以说明发展方向和变化速度。如增长速度  
### 3.2.3 统计量分析  
>用统计指标对定量数据进行统计描述。  
**1、集中趋势度量**  
1）均值，若不同成分所占的不同重要程度，可为数据集的每个值都添加一个权重；若数据中存在极端值或数据是偏态分布，那么均值便不能很好地度量数据的集中趋势，为了消除少数极端值的影响，可以使用截断均值或中位数来度量数据的及中国趋势。截断均值是去除高、低极端值之后的平均数。  
2）中位数：数据集中的值从小到大排序，位于中间的数。当数据个数为偶数，则是中间两个数的平均值。  
3）众数：指数据集中出现最频繁的值；众数不经常用来度量定性变量的中心位置，更适合用于定性变量。众数不具有唯一性，一般用于离散型变量而非连续性变量。  
**2、离中趋势度量**  
1）极差  
2）标准差：度量数据偏离均值的程度  
3）异变系数：度量标准差相对于均值的离中趋势；值为（标准差/平均值），主要用来比较两个或多个具有不同单位或不同波动幅度的数据集的离中趋势。  
4）四分位数间距：将所有数值从小到大排列分为四等份，处于第一个分割点的是下四分位，第二个分割点位位于中位数的位置，第三个分割点位置的数值是上四分位数。四分位间距是上四分位和下四分位的差值，其值越大，说明数据的变异程度越大，反之，越小。  
餐饮数据统计量分析例子如下：  

	import pandas as pd
	catering_scale='catering_sale.xls'
	data=pd.read_excel(catering_scale,index_col='日期')
	data=data[(data['销量']>400)&(data['销量']<5000)]# 过滤异常值
	
	statistics=data.describe()# 获取结果
	
	statistics.loc['range']=statistics.loc['max']-statistics.loc['min']# 添加极值
	statistics.loc['var']=statistics.loc['std']-statistics.loc['mean']# 添加异变系数
	statistics.loc['dis']=statistics.loc['75%']-statistics.loc['25%']# 四分位间距
	print(statistics)

### 3.2.4 周期性分析  
>探索某个变量是否随时间变化而呈现出某种周期变化趋势。时间尺度相对较长的周期性趋势有年度周期性趋势、季节性周期趋势，相对较短的有月度周期性趋势、周度周期性趋势，甚至更短的天、小时周期性趋势。
### 3.2.5 贡献度分析  
>贡献度分析又称帕累托分析，原理来自帕累托法则，又称20/80定律。重点改善盈利最高的前80%，可增加盈利。给出画帕累托的例子：  

	import pandas as pd
	import matplotlib.pyplot as plt
	
	catering_dish = 'catering_dish_profit.xls'
	data = pd.read_excel(catering_dish, index_col='菜品名')  # 读取信息
	print(data)
	data = data['盈利'].copy()
	
	data = data.sort_values(ascending=False)  # 对盈利倒序
	
	plt.figure()
	data.plot(kind='bar')
	p = 1.0 * data.cumsum() / data.sum()# 比例
	p.plot(color='r', secondary_y=True, style='-o', linewidth=2)
	plt.annotate(format(p[6], '.4%'), xy=(6, p[6]), xytext=(6 * 0.9, p[6] * 0.9),
	             arrowprops=dict(arrowstyle="->", connectionstyle="arc3,rad=.2"))  # 添加注释，即85%处的标记。这里包括了指定箭头样式。
	plt.show()

### 3.2.6 相关性分析 
>分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析。  
**1.直接绘制散点图**    
判断两个变量是否具有线性相关关系的最直观方法是直接绘制散点图。  
**2.绘制散点图矩阵**  
若需要同时考察多个变量间的相关关系时，一一绘制他们间的简单散点图是十分麻烦的。此时可利用散点图矩阵同时绘制各变量间的散点图，从而快速发现多个变量间的主要相关性。在多元线性回归有重要作用。  
**3.计算相关系数**    
为了更加准确地描述变量之间的线性相关程度，可通过计算相关系数进行相关分析。  
1）Pearson相关系数，一般用于分析两个连续性变量之间的关系，要求服从正态分布：  
![](http://img.my.csdn.net/uploads/201211/21/1353507674_8005.png)   
相关系数r的取值范围：-1<=r<=1;  
0<|r|<1表示存在不同程度线性相关：  
若0.3<|r|<=0.5 低度线性相关    
若0.5<|r|<=0.8 显著线性关系  
若|r|>0.8 高度线性关系  
2）Spearman秩相关系数，不服从正态分布的变量、分类或等级变量之间的关联性。   
只要两个变量具有严格单调的函数关系，那么他们就是完全Spearman相关，这与Pearson相关不同，Pearson相关只有在变量具有线性关系时才完全相关。研究表明，在正态分布假定的情况下，Spearman秩相关系数与Pearson相关系数在效率上等价，而对于连续测量数据，更适合用Pearson相关系数来进行分析。  
3）判定系数  
判定系数是相关系数的平方，用r<sup>2</sup> 表示；用来衡量回归方程对y的解释程度。判定系数取值范围：0<=r<sup>2</sup><=1。r<sup>2</sup>越接近1，表明x与y之间的相关性越强；反之月接近0，表明两个变量之间几乎没有直线相关关系。  

	# 不同菜品之间的关系
	import pandas as pd
	
	data=pd.read_excel('catering_sale_all.xls',index_col='日期') 
	print(data.corr())# 相关系数矩阵，即给出了任意两个菜之间的关系  Spearman(Pearman)
	print(data.corr()['百合酱蒸凤爪']) # 给出了百合酱蒸凤爪与其他任意菜之间的关系

## 3.3 Python主要数据探索函数  
* [python学习笔记](http://www.cnblogs.com/NSGUF/p/7459427.html) 
* [Matplotlib学习笔记](http://www.cnblogs.com/NSGUF/p/7406027.html)  
# 第4章 数据预处理  
>数据预处理的主要内容包括数据清洗、数据集成、数据变换和数据规约。在数据挖掘的过程中，数据预处理工作量占整个过程的60%：  
## 4.1 数据清洗  
>删除原始数据集中无关数据、重复数据、平滑噪声数据，筛选掉与挖掘主题无关的数据，处理缺失值、异常值等。  
### 4.1.1 缺失值处理  
方法有：删除记录、数据插补、不处理。其中数据插补常用方法有：均值/中位数/众数插补、使用固定值、最近临插补、回归方法、插值法。  
1）拉格朗日插值法  
![](http://upload.wikimedia.org/math/d/6/2/d62a2ffab8cf2363fcc1f26f388838d3.png)  
该方法结构紧凑，在理论分析中方便，但是当差值节点增减时，差值多项式就会随之变化，这在实际计算中十分不便。  
例子：  

	import pandas as pd
	from scipy.interpolate import lagrange
	
	inputfile = 'catering_sale.xls'
	outputfile = 'sales.xls'
	
	data = pd.read_excel(inputfile)  # 读取excel
	data.loc[(data['销量'] < 400) | (data['销量'] > 5000), '销量'] = None  # 异常值变为空值
	
	def ployinterp_column(s, n, k=5):  # 默认是前后5个
	    y = s[list(range(n - k, n)) + list(range(n + 1, n + 1 + k))]  # 取数，n的前后5个，这里有可能取到不存在的下标，为空
	    y = y[y.notnull()]  # 如果y里面有空值的话就去掉
	    return lagrange(y.index, list(y))(n)  # 最后的括号就是我们要插值的n
	
	for i in data.columns:
	    if i == '日期':
	        continue
	    for j in range(len(data)):
	        if (data[i].isnull())[j]:  # 空值进行插值
	            data.loc[j, i] = ployinterp_column(data[i], j)
	data.to_excel(outputfile)  

### 4.1.2 异常值处理  
* 删除含有异常值的记录  
* 视为缺失值再处理（利用现有数据对异常值进行填补）  
* 平均值修正，前后两个观察值得平均 
* 不处理
## 4.2 数据集成  
>数据挖掘需要的数据往往分布在不同的数据源中，数据集成就是将多个数据源合并存放在一个一致的数据存储中的过程。  
>在数据集成时，来自多个数据源的现实世界实体的表达形式不一样，有可能不匹配，要考虑实体识别问题和属性冗余问题，从而将数据源在最底层加以转换、提炼和集成。  
### 4.2.1 实体识别  
>实体识别是指从不同数据源识别现实世界的实体，它的任务是统一不同数据源的矛盾之处，常见形式如下：  

* 同名异义
* 异名同义
* 单位不统一   
### 4.2.2 冗余属性识别  
>仔细整合不同源数据能减少甚至避免数据冗余与不一致，从而提高数据挖掘的速度和质量。有些冗余属性可用相关分析检测，给定两个数值型的属性A和B，根据其属性值，用相关系数度量一个属性在多大程度上蕴含另一个属性，可见3.2.6。冗余形式如下：  

* 同一属性多次出现  
* 同一属性命名不一致导致重复

## 4.3 数据变换  
>对数据进行规范化处理，将数据转换成“适当的”形式，以适用于挖掘任务及算法的需要。
### 4.3.1 简单函数变换  
>简单函数变换是对原始数据进行某些数学函数变换，常用的变换包括平方、开方、取对数、差分运算等。常用来将不具有正态分布的数据变换成具有正态分布的数据。在时间序列分析中，有时简单的对数变换或者差分运算就可以将非平稳序列转换成平稳序列。在数据挖掘中，简单的函数变换可能更有必要，如个人年收入取值范围为10000到10亿，区间较大，可使用对数变换进行压缩。  
### 4.3.2 规范化  
>不同评价指标往往具有不同的量纲，数值间的差别可能很大，不进行处理可能会影响到数据分析的结果。为了消除指标之间的量纲和取值范围差异的影响，需要进行标准化处理，将数据按照比例进行缩放，使之落入一个特定的区域，便于进行综合分析。对于基于距离的挖掘算法尤为重要。方法有以下几种：  

* 最小-最大规范化  
>也称为离差标准化，对原始数据的线性变换，将数值映射得到[0,1]之间，公式为：x<sup>*</sup>=(x-min)/(max-min)；该方法保留了原来数据中存在的关系，消除量纲和数据取值范围影响的最简单方法。这种处理方法的缺点是若数值集中且数值很大，则规范化后各值会接近于0，并且将会相差不大，若将来遇到超过目前属性[min,max]取值范围的时候，会引起系统出错，需要重新确定min和max。 
 
* 零-均值规范化  
>也称为标准差标准化，经过处理的数据的均值为0，标准差为1。转换公式为：x<sup>*</sup>=(x-原始数据均值）/原始数据的标准差，这种方法是目前用的最多的数据标准化方法。    

* 小数定标规范化  
>通过移动属性值的小数位数，将属性值映射到[-1,1]之间，移动的小数位数取决于属性值绝对值的最大值。转换公式：x<sup>*</sup>=x/10<sup>k</sup>  

	import pandas as pd
	import numpy as np
	datafile='normalization_data.xls'
	data=pd.read_excel(datafile,header=None)
	# 最小-最大规范化
	print((data-data.min())/(data.max()-data.min()))
	# 零-均值规范化
	print((data-data.mean())/data.std())
	# 小数定标规范化
	print(data/10**np.ceil(np.log10(data.abs().max())))
### 4.3.3 连续属性离散化  
>一些数据挖掘算法，特别是某些分类算法（如ID3算法、Apriori算法等）要求数据是分类属性形式，这样常常需要将连续属性表换成分类属性，即连续属性离散化    

1.离散化的过程  
>连续属性的离散化是在数据的取值范围内设定若干个离散的划分点，将取值范围划分为一些离散化的区间，最后用不同的符号或整数值代表落在每个字区间中的数据值。所以，离散化涉及两个子任务：确定分类数以及如何将连续属性值映射到这些分类值。  

2.常用的离散化方法   

* 等宽法
>将属性的值域分成具有相同宽度的区间，区间的个数由数据本身的特点决定，或者由用户指定，类似于制作频率分布表。  

* 等频法  
>将相同数量的记录放进每个区间。  

* 基于聚类分析的方法  
>首先将连续属性的值用聚类算法（如K-Means算法）进行聚类，然后再将聚类得到的簇进行处理，合并到一个簇的连续属性值并做同一标记。聚类分析的离散化方法也需要用户指定簇的个数。  
> 等宽法和等频法方法简单，易操作，但都需要人为的规划好区间个数；等宽法缺点在于它对力群点比较敏感，倾向于不均匀地把属性值分布到各个区间。有些区间包含许多数据，有些区间数据极少。等频虽然避免了该问题，但可能将相同的数据值分到不同的区间以满足每个区间中固定的数据个数。  
### 4.3.4 属性构造  
>为了提取更有用的信息，挖掘更深层次的模式，提高挖掘结果的精度，我们需要利用已有的属性集构建新的属性，并加入到现有的属性中。  

	import pandas as pd
	inputfile='electricity_data.xls'
	outfile='electricity_data_out.xls'
	data=pd.read_excel(inputfile)
	data['线损率']=(data['供入电量']-data['供出电量'])/data['供入电量']
	data.to_excel(outfile,index=False)# index表示行号是否显示

### 4.3.5 小波变换  
>小波变换具有多分辨率的特点，在时域和频域都具有表征信号局部特征的能力，通过伸缩和平移等运算过程对信号进行多尺度聚焦分析，提供了一种非平稳信号的时频分析手段，可以由粗及细地逐步观察信号，从中提取有用信息。能刻画某个问题的特征量往往是隐含在一个信号中的某个或者某个分量中，小波变换可以把非平稳信号分解为表达层次不同、不同频带信息的数据序列，即小波系数。选取适当的小波系数，即完成了信号的特征提取。  

1、基于小波变换的特征提取方法    

* 基于小波变换的多尺度空间能量分布特征提取方法  
>各尺度空间内的平滑信号和细节信号能提供原始信号的时频局域信息，特别是能提供不同频段上信号的不同信息。把不同分解尺度上的信息能量求解出来，就可将这些能量尺度顺序排列，形成特征向量供识别用。

* 基于小波变换的多尺度空间的模极大值特征提取方法  
>利用小波变换的信号局域化分析能力，求解小波变化的模极大值特征来检测信号的局部奇异性，将小波变换模极大值的尺度参数s、平移参数t及其幅值作为目标的特征量。

* 基于小波包变换的特征提取
>利用小波分解，可将时域随机信号序列映射为尺度预各子空间内的随机系数序列，按小波包分解得到的最佳子空间内随机系数序列的不确定性程度最低，将最佳子空间的熵值即最佳子空间在完整二叉树中的位置参数作为特征量，可用于目标识别。

* 基于适应性小波神经网络的特征提取方法
>基于适应性小波神经网络的特征提取方法可以把信号通过分析小波拟合表示，进行特征提取。

2、小波基函数  
>小波基函数是一种具有局部支集的函数，并且平均值为0。常用有：Haar小波基，db系列小波基等。

3、小波变换
>对小波基函数进行伸缩和平移变换：

4、基于小波变换的多尺度空间能量分布特征提取方法  
>利用小波变换可以对声波信号进行特征特区，提取出可以代表声波信号的向量数据，即完成从声波信号到特征向量数据的变换。  
	inputfile='leleccum.mat'
	
	from scipy.io import loadmat
	mat=loadmat(inputfile)# mat位python专属格式，需要用loadmat读取
	signal=mat['leleccum'][0]
	import pywt
	coeffs=pywt.wavedec(signal,'bior3.7',level=5)# 小波变换特征提取，返回level+1个数字，第一个数组为逼近系数数组，后面的依次为细节系数数组
	print(coeffs)

## 4.4 数据规约
>数据规约产生更小但担保原数据完整性的新数据集。其意义在于：

* 降低无效、错误数据对建模的影响，提高建模的准确性；
* 少量且具代表性的数据将大幅缩减数据挖掘所需要的时间；
* 降低存储数据的成本。

### 4.4.1 属性规约
>属性规约通过属性合并来创建新属性维数，或者直接通过删除不相关的属性（维）来减少数据维数，从而提高数据挖掘的效率、降低计算成本。属性规约的目标是寻找出最小的属性子集并确保新数据子集的概率分布尽可能地接近原来数据集的概率分布。  
常用方法：  

* 合并属性
>将一些旧属性合为新属性  

* 逐步向前选择  
>从一个空集开始，每次从原来的属性集合中选择一个当前最优的属性并添加到当前属性子集中，直到无法选择出最优属性或满足一定阙值约束为止。  

* 逐步向后选择
>从一个全属性集开始，每次从当前属性集合中选择一个当前最差的属性并将其从当前属性子集中剔除，直到无法选择出最差属性或满足一定阙值约束为止。

* 决策树归纳  
>利用决策树的归纳方法对初始数据进行分类归纳学习，获得初始决策树，所有没有出现在决策树上的属性均可认为是无关属性，因此将这个属性从初始集合中删除即可获得较优属性子集。

* 主成分分析
>用较少的变量去解释原数据中的大部分变量，即将许多相关性很高的变量转换成彼此相互独立或不相关的变量。

>逐渐向前、向后、决策树属于直接删除不想关属性方法。主成分分析是一种用于连续属性的数据降维方法，它构造了原始数据的一个正交变换，新空间的基底去除了原始空间基底下数据的相关性，只需使用少数新变量就能解释原始数据中的大部分变量。

	import pandas as pd
	from sklearn.decomposition import PCA
	inputfile = 'principal_component.xls'
	outputfile = 'dimention_reducted.xls'
	data=pd.read_excel(inputfile,header=None)
	
	pca=PCA(n_components=None,copy=True,whiten=False)# n_components表示算法中保留的主成分个数，int或string，即保留下来的特征个数，缺省时默认为None，所有成分被保留，为string时，如n_components=‘mle'表示自动选择特征个数n，使得满足所有要求的方差百分比，copy，默认为True，表示是否在运行算法时，将原始训练数据复制一份，若为True，则运行PCA算法后，原始数据不会有任何改变，因为是在副本上运行，若为False，则运行PCA算法后，原始数据会改变，即在原始数据中进行降维。whiten表是否白化，使得每个特征具有相同的方差。
	
	pca.fit(data)
	print(pca.components_)# 返回模型的各个特征向量
	print(pca.explained_variance_ratio_)# 返回各个成分各自的方差百分比
	
	pca=PCA(3)
	pca.fit(data)
	low_d=pca.transform(data)# 降低维度
	pd.DataFrame(low_d).to_excel(outputfile) # 保存结果
	pca.inverse_transform(low_d)# 复原数据
### 4.4.2 数值规约
>数值规约指通过选择代替的、较小的数据来减少数据量，包括有参数方法和无参数方法两类。有参数方法是使用一个模型来评估数据，只需存放参数，而不需要存放实际数据，例如回归（线性回归和多元回归）和对数线性模型（近似离散属性集中的多维概率分布）。无参数方法就需要存放实际数据，例如直方图、聚类、抽样。

* 直方图
* 聚类
* 抽样
* 参数回归

## 4.5 Python主要数据预处理函数
* interpolate
>一维、高维数据插值，包含了大量的插值函数如拉格朗日插值、样条插值、高维插值等；使用：from scipy.interpolate import *引入。

* unique
>去除数据的重复元素，得到单值元素列表；使用格式：np.unique(一维数据)或Series.unique()

* isnull/notnull
>判断元素是否空值/非空值；使用格式：Series.isnull()/Series.notnull()，可通过Series[Series.isnul()]找出空值。

* random
>生成随机矩阵；使用格式：np.random.rand(k,m,n),k*m*n的随机均匀分布在(0,1)上，np.random.randn(k,m,n)，k*m*n随机标准正态分布；

* PCA
>对指标变量矩阵进行主成分分析，可看4.4.1小节。
# 第5章 挖掘建模
## 5.1 分类与预测
>分类和预测是预测问题的两种主要类型，分类主要是预测分类标号（离散属性），而预测主要是建立连续值函数模型，预测给定自变量对应的因变量的值。

### 5.1.1 实现过程
>1）分类：分类是构造一个分类模型，输入样本的属性值，输出对应的类别，将每个样本映射到预先定义好的类别。分类模型定义在已有类标记的数据集上，模型在已有样本上的准确率可以方便的计算，所以分类属于监督的学习。  
>2）预测：指建立两种或两种以上变量间互相依赖的函数模型，然后进行预测或控制。  

### 5.1.2 常用的分类与预测算法
* 回归分析：确定预测属性（数值型）与其他变量间相互依赖的定量关系最常用统计学方法。
* 决策树：采用自顶向下的递归方法，在内部节点进行属性值得比较，并根据不同的属性值从该节点向下分支，最终得到的叶节点是学习划分的类。
* 人工神经网络：模仿大脑神经网络结构和功能而建立的信息处理系统，表示神经网络的输入与输出变量之间关系的模型。
* 贝叶斯网络：Bayes方法的扩展。
* 支持向量机：一种通过某种非线性映射，把低维的非线性可分转化为高维的线性可分，在高维空间进行线性分析的算法。

### 5.1.3 回归分析
>回归分析是通过建立模型来研究变量之间相互关系的密切程度、结构状态及进行模型预测的一种有效供给，在工商管理、经济、社会、医学和生物学等领域应用十分广泛。主要回归模型分类如下：  

* 线性回归
>因变量与自变量是线性关系，可用最小二乘法求解模型系数。

* 非线性模型
>因变量与自变量之间不都是线性关系，若非线性关系可通过简单的函数变换转化成线性关系，用线性回归的思想求解，若不能转换，则可用非线性最小二乘方法求解。

* Logistic回归
>因变量一般有1和0（是否）两种取值，在广义线性回归模型的特例，利用Logistic函数将因变量的取值范围控制在0和1之间，表示取值为1的概率。属于概率型非线性回归。

* 岭回归
>参与建模的自变量之间具有多重共线性，是一种改进最小二乘估计的方法。

* 主成分回归
>参与建模的自变量之间具有多重共线性，是对最小二乘法的一种改进，它是参数估计的一种有偏估计，可以消除自变量之间的多重共线性。

	# 逻辑回归
	import pandas as pd
	
	filename = 'bankloan.xls'
	from sklearn.linear_model import RandomizedLogisticRegression as RLR# 随机逻辑回归模型
	from sklearn.linear_model import LogisticRegression as LR# 逻辑回归模型
	
	data = pd.read_excel(filename)
	x = data.iloc[:, :8].as_matrix()
	y = data.iloc[:, 8].as_matrix()
	
	rlr = RLR()
	rlr.fit(x, y)
	print('score：', ','.join(data.columns[rlr.get_support()]))# 通过随机逻辑回归模型找出特征
	
	x=data[data.columns[rlr.get_support()]].as_matrix()
	lr=LR()
	lr.fit(x,y)
	print('正确率：',lr.score(x,y))# 获取训练结果
>递归特征消除的主要思想是反复的构建模型，然后选出最好/最差的特征然后重复并排序，遍历完所有的特征，这是一种寻找最优特征子集的贪心算法

### 5.1.4 决策树
>决策树是一树状结构，它的每一个叶节点对应一个分类，非叶节点对应着在某个属性上的划分，根据样本在该属性上的不同取值将其划分成若干个子集。对于非纯的叶节点，多数类的标号给出到大这个节点的样本所属的类。构造决策树的核心问题是在每一步如何选择适当的属性对样本做拆分。对一个分类问题，从已知类标记的训练样本中学习并构造出决策树是一个自上而下，分而治之的过程。  

* ID3算法  
>其核心是在决策树的各级节点上，使用信息增益方法作为属性的选择标准，来帮助确定生成每个节点时所应采用的合适属性  

* C4.5算法  
>C4.5决策树生成算法相对于ID3算法的重要改进是使用信息增益率来选择节点属性。C4.5算法可以克服ID3算法存在的不足；ID3算法只适用于离散的描述属性，而C4.5算法既能够处理离散的描述属性，也可以处理连续的描述属性。

* CART算法  
>CART决策树是一种十分有效的非参数分类和回归方法，通过构建树、修剪树、评估树来构建一个二叉树。当终结点是连续变量时，该树为回归树，当终结点是分类变量时，该树为分类树。  

>ID3算法具体流程  
1.对当前样本集合，计算所有属性的信息增益。
2.选择信息增益最大的属性作为测试属性，把测试属性取值相同的样本划分为同一个子样本集；  
3.若子样本集的类别属性只含有单个属性，则分支为叶子节点，判断其属性值并标上相应的符号，然后返回调用处；否则对子样本集递归调用本算法。  

>由于ID3决策树算法采用了信息增益作为选择测试属性的标准，会偏向于选择取值较多的，即所谓高度分支属性，而这类属性并不一定是最优的属性，同事ID3决策树算法只能处理离散属性，对于连续型属性，在分类之前需对其进行离散化，为了解决倾向于选择高度分支属性的问题，人们采用信息增益率作为选择测试属性的标准，这样便得到了C4.5决策树算法，此外还有CART算法、SLIQ算法、SPRINT算法、PUBLIC算法等。  
下面是一个决策树算法预测销量高低的例子：
	from sklearn.tree import DecisionTreeClassifier as DTC
	from sklearn.tree import export_graphviz
	from sklearn.externals.six import StringIO
	
	filename = 'sales_data.xls'
	data = pd.read_excel(filename, index_col='序号')
	data[data == '好'] = 1
	data[data == '是'] = 1
	data[data == '高'] = 1
	data[data != 1] = -1
	
	x = data.iloc[:, :3].as_matrix().astype(int)
	y = data.iloc[:, 3].as_matrix().astype(int)
	
	dtc = DTC(criterion='entropy')
	dtc.fit(x, y)
	x = pd.DataFrame(x)
	with open('tree.dot', 'w') as f:
	    f = export_graphviz(dtc, feature_names=x.columns, out_file=f)

### 5.1.5 人工神经网络  
>人工神经网络是模拟生物神经网络进行信息处理的一种数学模型。人工神经网络的学习称为训练，指的是神经网络在受到外界环境的刺激下调整神经网络的参数，是神经网络以一种新的方式对外部环境做出反应的一个过程。在分类与预测中，人工神经网络主要使用有指导的学习方式，即根据给定的训练样本，调整人工神经网络的参数以使网络输出接近于已知样本类标记或其他形式的因变量。
  
激活函数分类：  

* 域值函数（阶梯函数）   
 
>当自变量小于0时，函数输出为0，当自变量大于0时，函数输出为1。  

* 分段线性函数  
>当自变量小于-1时，输出为-1，当大于-1小于1时，则自变量等于因变量，当大于1时，为1。  

* 非线性函数  
>自变量为0到1之间，公式为：f=1/(1+e<sup>-v</sup>)

* Relu函数  
>具有计算简单，效果更佳的特点，目前已经有取代其他激活函数的趋势。当自变量大于或等于0时，因变量等于自变量，当小于0时，则等于0。

### 5.1.6 分类与预测算法评价
>分类与预测模型对训练集进行预测而得到的准确率并不能很好的反映预测模型的性能，所以需要对其进行有效判别。使用方法如下：

* 绝对误差与相对误差
* 平均绝对误差
* 均方误差
* 均方根误差
* 平均绝对百分误差
* Kappa统计
* 识别准确度
* 识别精确度
* 反馈率
* ROC曲线
* 混淆矩阵

### 5.1.7 Python分类预测模型特点
* 逻辑回归
>比较基础的线性分类模型，很多时候是简单有效的选择，sklearn.linear_model

* SVM
>强大的模型，可用来回归、预测、分类等，而根据选取不同的核函数，模型可以线性/非线性,sklearn.svm

* 决策树
>基于分类讨论，逐步细化思想的分类模型，模型直观易解释,sklearn.tree

* 随机森林
>思想和决策树相似，精度通常比决策树要高，缺点是由于其随机性，丧失了决策树的可解释性,sklearn.ensemble

* 朴素贝叶斯
>基于概率思想的简单有效的分类模型，能够给出容易理解的概率解释,sklearn.naive_bayes

* 神经网络
>具有强大的拟合能力，可用于拟合、分类等，它有很多增强版本，如递归神经网络、卷积神经网络、自编码器等，这些是深度学习的基础。Keras

>不管是Scikit-Learn还是Keras中，建模第一步都是先建立一个对象，对象是空白的，然后进一步训练，设置模型参数，接着fit方法进行训练，最后predict方法预测结果。还有如score方法对模型评估。

## 5.2 聚类分析
>与分类不同，聚类分析是在没有定划分类别的情况下，根据数据相似度进行样本分组的一种方法，与分类模型需要使用有类标记样本构成的训练数据不同，聚类模型可以建立在无类标记的数据上，是一种非监督的学习算法。聚类输入是一组未标记的样本，聚类根据数据本身的距离或相似度将其划分为若干组，划分的原则是组内距离最小化而组间（外部）距离最大化。
### 5.2.1 分析算法
>常用聚类方法如下：

* 划分（分裂）方法  
>K-Means算法（K-平均），K-MEDOIDS算法（K-中心点）、CLARANS算法（基于选择的算法）

* 层次分析方法
>BIRCH算法（平衡迭代规划和聚类）、CURE算法（代表点聚类）、CHAMELEON算法（动态模型）

* 基于密度的方法
>DBSCAN算法那（基于高密度连接区域）、DENCLUR算法（密度分布函数）、OPTICS算法（对象排序识别）


* 基于网格的方法
>STING算法（统计信息网格）、CLIOUE算法（聚类高维空间）、WAVE-CLUSTER算法（小波变换）

* 基于模型的方法
>统计学方法、神经网络方法  

常用聚类分析算法  

* K-Means算法
>也称为快速聚类法，在最小化误差函数的基础上将数据划分为预定的类数K。该算法原理简单并便于处理大量数据。

* K-中心点
>对孤立点的敏感性，K-中心点算法不采用簇中对象的平均值作为簇中心，而选用簇中离平均值最近的对象作为簇中心。

* 系统聚类
>也称为多层次聚类，分类的单位由高到低呈树形结构，且所处的位置越低，其所包含的对象就越少，但这些对象间的共同特征越多，该聚类方法只适合在小数据量的时候使用，数据量大的时候速度会非常慢。

### 5.2.2 K-Means聚类算法
>K-Means算法是典型的基于距离的非层次聚类算法，在最小化误差函数的基础上将数据划分为预定的类数K，采用距离作为相似性的评价指标，即认为两个对象的距离越近，其相似度越大。例子如下：

	inputfile = 'consumption_data.xls'
	outputfile = 'data-type.xls'
	
	k = 3
	iteration = 500
	data = pd.read_excel(inputfile, index_col='Id')
	data_zs = 1.0 * (data - data.mean()) / data.std()  # 标准化
	
	from sklearn.cluster import KMeans
	
	model = KMeans(n_clusters=k, init='k-means++', n_init=4)  # 分为k类
	model.fit(data_zs)
	# 简单打印结果
	r1 = pd.Series(model.labels_).value_counts()  # 统计各个类别的数目
	r2 = pd.DataFrame(model.cluster_centers_)  # 找出聚类中心
	r = pd.concat([r2, r1], axis=1)  # 横向连接（0是纵向），得到聚类中心对应的类别下的数目
	r.columns = list(data.columns) + ['类别数目']  # 重命名表头
	print(r)
	
	# 详细输出原始数据及其类别
	r = pd.concat([data, pd.Series(model.labels_, index=data.index)], axis=1)  # 详细输出每个样本对应的类别
	r.columns = list(data.columns) + [u'聚类类别']  # 重命名表头
	r.to_excel(outputfile)  # 保存结果
	
	
	def density_plot(data):  # 自定义作图函数
	    import matplotlib.pyplot as plt
	    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
	    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
	    p = data.plot(kind='kde', linewidth=2, subplots=True, sharex=False)
	    [p[i].set_ylabel('密度') for i in range(k)]
	    plt.legend()
	    return plt
	
	
	pic_output = 'pd_'  # 概率密度图文件名前缀
	for i in range(k):
	    density_plot(data[r[u'聚类类别'] == i]).savefig(u'%s%s.png' % (pic_output, i))
### 5.2.3 聚类分析算法评价
>聚类分析仅根据样本数据本身将样本分组，其目标是实现组内的对象相互之间是相似的，而不同组中的对象是不同的，组内的相似性越大，组间差距也越大，聚类效果越好。如下方法：

* purity评价法
>计算正确的比例

* RI评价法
>RI=（R+W）/（R+M+D+W）  R表聚在一类的两个对象被正确分类了，W指不应该被聚在一类的两个对象被正确分开了，M指不应该放在一类的对象被错误放在一类，D指不应该分开的对象被错误分开了。

* F值评价法
>基于RI方法衍生，RI方法就是把准确率和召回率看的同等重要，事实上，可能会需要某种特征。

### 5.2.4 Python主要聚类分析算法
* KMeans
* AffinityPropagation
>几乎优于其他所有方法，不需要指定聚类数，但是运行效率较低

* MeanShift
>均值漂移聚类算法

* SpectralClustering
>谱聚类，具有效果比K均值好，速度比K均值快等特点

* AgglomerativeClustering
>层次聚类，给出一颗聚类层次数

* DBSCAN
>具有噪声的基于密度的聚类方法

* BIRCH
>综合的层次聚类算法，可以处理大规模数据的聚类

>模型的使用方法大同小异，基本都是先用对应的函数建模，然后.fit()训练，再用.label_获得数据标签或用.predict()方法预测新的输入标签。


>聚类可视化工具---TSNE
	
	from sklearn.manifold import TSNE
	
	tsne = TSNE()
	tsne.fit_transform(data_zs)  # 进行数据降维
	tsne = pd.DataFrame(tsne.embedding_, index=data_zs.index)  # 转换数据格式
	
	import matplotlib.pyplot as plt
	
	plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
	plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
	
	# 不同类别用不同颜色和样式绘图
	d = tsne[r[u'聚类类别'] == 0]
	plt.plot(d[0], d[1], 'r.')
	d = tsne[r[u'聚类类别'] == 1]
	plt.plot(d[0], d[1], 'go')
	d = tsne[r[u'聚类类别'] == 2]
	plt.plot(d[0], d[1], 'b*')
	plt.show()

## 5.3 关联规则
>关联规则分析目的是在一个数据集中找出各项之间的关联关系，而这种关系并没有在数据中直接表示出来。
### 5.3.1 常用关联规则算法
* Apriori
>最常用也是最经典的挖掘频繁项集的算法，其核心思想是通过连接产生候选项及其支持度然后通过剪枝生成频繁项集。

* FP-Tree
>针对Apriori算法的固有的多次扫描事物的数据集的缺陷，提出的不产生候选频繁项集的方法。Apriori和FP-Tree都是寻找频繁项集的算法。

* Eclat算法
>Eclat算法是一种深度优先算法，采用垂直数据表示形式，在概念格理论的基础上利用基于前缀的等价关系将搜索空间划分为较小的子空间。

* 灰色关联法
>分析和确定各因素之间的影响程度或是若干个子因素（子序列）对主因素（母序列）的贡献度而进行的一种分析方法。

###
5.2.2 Apriori算法
	# 自定义连接函数，用于实现L_{k-1}到C_k的连接
	def connect_string(x, ms):
	    x = list(map(lambda i: sorted(i.split(ms)), x))
	    l = len(x[0])
	    r = []
	    for i in range(len(x)):
	        for j in range(i, len(x)):
	            if x[i][:l - 1] == x[j][:l - 1] and x[i][l - 1] != x[j][l - 1]:
	                r.append(x[i][:l - 1] + sorted([x[j][l - 1], x[i][l - 1]]))
	    return r
	
	
	# 寻找关联规则的函数
	def find_rule(d, support, confidence, ms='--'):
	    result = pd.DataFrame(index=['support', 'confidence'])  # 定义输出结果
	
	    support_series = 1.0 * d.sum() / len(d)  # 支持度序列
	    column = list(support_series[support_series > support].index)  # 初步根据支持度筛选
	    k = 0
	
	    while len(column) > 1:
	        k = k + 1
	        print('\n正在进行第%s次搜索...' % k)
	        column = connect_string(column, ms)
	        print('数目：%s...' % len(column))
	        sf = lambda i: d[i].prod(axis=1, numeric_only=True)  # 新一批支持度的计算函数
	
	        # 创建连接数据，这一步耗时、耗内存最严重。当数据集较大时，可以考虑并行运算优化。
	        d_2 = pd.DataFrame(list(map(sf, column)), index=[ms.join(i) for i in column]).T
	
	        support_series_2 = 1.0 * d_2[[ms.join(i) for i in column]].sum() / len(d)  # 计算连接后的支持度
	        column = list(support_series_2[support_series_2 > support].index)  # 新一轮支持度筛选
	        support_series = support_series.append(support_series_2)
	        column2 = []
	
	        for i in column:  # 遍历可能的推理，如{A,B,C}究竟是A+B-->C还是B+C-->A还是C+A-->B？
	            i = i.split(ms)
	            for j in range(len(i)):
	                column2.append(i[:j] + i[j + 1:] + i[j:j + 1])
	
	        cofidence_series = pd.Series(index=[ms.join(i) for i in column2])  # 定义置信度序列
	
	        for i in column2:  # 计算置信度序列
	            cofidence_series[ms.join(i)] = support_series[ms.join(sorted(i))] / support_series[ms.join(i[:len(i) - 1])]
	
	        for i in cofidence_series[cofidence_series > confidence].index:  # 置信度筛选
	            result[i] = 0.0
	            result[i]['confidence'] = cofidence_series[i]
	            result[i]['support'] = support_series[ms.join(sorted(i.split(ms)))]
	    result = result.T.sort_values(['confidence', 'support'], ascending=False)  # 结果整理，输出
	    print('\n结果为：')
	    print(result)
	    return result
	
	
	inputfile = 'menu_orders.xls'
	outputfile = 'apriori_rules.xls'
	
	data = pd.read_excel(inputfile, header=None)
	ct = lambda x: pd.Series(1, index=x[pd.notnull(x)])
	b = map(ct, data.as_matrix())
	data = pd.DataFrame(list(b)).fillna(0)
	print('转换ok')
	del b
	support = 0.2
	confidence = 0.5
	ms = '---'
	find_rule(data, support, confidence, ms).to_excel(outputfile)

## 5.4 时序模式
>目的是给定一个已被观测了的时间序列，预测该序列的未来值。
### 5.4.1 时间序列算法
>常用时间序列模型如下：
* 平滑法
>常用于趋势分析和预测，利用修匀技术，削弱短期随机波动对序列的影响，使序列平滑化。根据所用平滑技术的不同，可具体分为移动平均法和指数平滑法。

* 趋势拟合法
>把时间作为自变量，相应的序列观察值作为因变量，建立回归模型，根据序列的特征，可具体分为线性拟合和曲线拟合。

* 组合模型
>时间序列的变化主要受长期趋势、季节变动、周期变动和不规则变动这四个因素影响，根据序列的特点，可以构建加法模型和乘法模型。

* AR模型
* MA模型
* ARMA模型
* ARIMA模型
* ARCH模型
* GARCH模型及其衍生模型

### 5.4.2 时间序列的预处理
>拿到一个观察值序列后，首先要对它的纯随机性和平稳性进行检验，这两个重要的检验称为序列的预处理。根据检验结果可以将序列分为不同的类型，对不同类型的序列会采取不同的分析方法。  
>对于纯随机序列（白噪声序列），序列的各项之间没有任何相关关系，序列在进行完全无须的随机波动，可以终止对该序列的分析。白噪声序列是没有信息可提取的平稳序列。  
>对于平稳非白噪声序列，它的均值和方差是常熟，现已有一套非常成熟的平稳序列的建模方法。通常是建立一个线性模型来拟合该序列的发展，借此提取该序列的有用信息。ARMA模型是最常用的平稳序列拟合模型。   
>对于非平稳序列，由于他的均值和方差不稳定，处理方法一般是将其转变成平稳序列，这样就可应用有关平稳时间序列的分析方法，如建立ARMA模型来进行相应的研究。如果一个时间序列经差分运算后具有平稳性，则该序列为差分平稳序列，可使用ARIMA模型进行分析。   

1.平稳性检验  
>检验有两种方法：一种是根据时序图和自相关图的特征判断的图检验，操作简单、应用广泛、缺点是带有主观性。另一种是构造检验统计量进行检验的方法，目前最常用的方法是单位根检验。  

2.纯随机性检验  
>也称为白噪声检验，一般是构造检验统计量来检验序列的纯随机性，常用的检验统计量有Q统计量、LB统计量，由样本个延迟期数的自相关系数可以计算得到检验统计量，然后计算出对应的P值，如果p值显著大于显著性水平a，则表示该序列不能拒绝纯随机性的原假设，可以停止对该序列的分析。
### 5.4.3 平稳时间序列分析  
>ARMA模型的全称是自回归移动平均模型，它是目前最常用的拟合平稳序列的模型。它可细分为AR模型、MA模型和ARMA模型三大类，都可看做多元线性回归模型。  
### 5.4.4 非平稳时间序列分析
>可分为确定性因素分解的时序分析和随机时序分析两大类。

	discfile = 'arima_data.xls'
	forecastnum = 5
	
	# 读取数据，指定日期列为指标，Pandas自动将“日期”列识别为Datetime格式
	data = pd.read_excel(discfile, index_col='日期')
	
	# 时序图
	import matplotlib.pyplot as plt
	
	plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
	plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
	data.plot()
	plt.show()
	
	# 自相关图
	from statsmodels.graphics.tsaplots import plot_acf
	
	plot_acf(data).show()
	
	# 平稳性检测
	from statsmodels.tsa.stattools import adfuller as ADF
	
	print('原始序列的ADF检验结果为：', ADF(data['销量']))
	# 返回值依次为adf、pvalue、usedlag、nobs、critical values、icbest、regresults、resstore
	
	# 差分后的结果
	D_data = data.diff().dropna()
	D_data.columns = ['销量差分']
	D_data.plot()  # 时序图
	plt.show()
	plot_acf(D_data).show()  # 自相关图
	from statsmodels.graphics.tsaplots import plot_pacf
	
	plot_pacf(D_data).show()  # 偏自相关图
	print('差分序列的ADF检验结果为：', ADF(D_data['销量差分']))  # 平稳性检测
	
	# 白噪声检验
	from statsmodels.stats.diagnostic import acorr_ljungbox
	
	print('差分序列的白噪声检验结果为：', acorr_ljungbox(D_data, lags=1))  # 返回统计量和p值
	
	from statsmodels.tsa.arima_model import ARIMA
	
	data['销量'] = data['销量'].astype(float)
	# 定阶
	pmax = int(len(D_data) / 10)  # 一般阶数不超过length/10
	qmax = int(len(D_data) / 10)  # 一般阶数不超过length/10
	bic_matrix = []  # bic矩阵
	for p in range(pmax + 1):
	    tmp = []
	    for q in range(qmax + 1):
	        try:  # 存在部分报错，所以用try来跳过报错。
	            tmp.append(ARIMA(data, (p, 1, q)).fit().bic)
	        except:
	            tmp.append(None)
	    bic_matrix.append(tmp)
	
	bic_matrix = pd.DataFrame(bic_matrix)  # 从中可以找出最小值
	
	p, q = bic_matrix.stack().idxmin()  # 先用stack展平，然后用idxmin找出最小值位置。
	print('BIC最小的p值和q值为：%s、%s' % (p, q))
	model = ARIMA(data, (p, 1, q)).fit()  # 建立ARIMA(0, 1, 1)模型
	model.summary2()  # 给出一份模型报告
	model.forecast(5)  # 作为期5天的预测，返回预测结果、标准误差、置信区间。

### 5.4.5 Python主要时序模式算法
>算法主要是ARIMA算法，在使用该模型时，需要进行一系列的判别操作，主要包含：平稳性检验、白噪声检验、是否差分、AIC和BIC指标值、模型定阶，最后再做预测。时序模式算法函数列表如下：

* acf()：计算自相关系数
* plot_acf()：画自相关系数图
* pacf()/plot_pacf()：计算偏相关系数/画偏相关系数图
* adfuller():对观测值序列进行单位根验证
* diff()：对观测值序列进行差分计算
* arima：设置时序模式的建模参数，创建ARIMA时序模型
* summary()/summary2()：生成已有模型的报告
* aic/bic/hqic：计算ARIMA模型的AIC、BIC、HQIC指标值
* forecast()：用得到的时序模型进行预测
* acorr_ljungbox()检测是否为白噪声序列

## 5.5 离群点检测
>目的是发现与大部分其他对象显著不同的对象。大部分数据挖掘方法都将这种差异信息视为噪声并丢弃，然而在一些应用中，罕见的数据可能蕴含着更大的研究价值。  

1.离群点的成因：数据来源不同的类、自然变异、数据测量和收集有误。  
2.类别如下：

<table>
<tr>
<td>分类标准</td>
<td>分类名称</td>
<td>分类描述</td>
</tr>
<tr>
<td>从数据范围</td>
<td>全局离群点和局部离群点</td>
<td>从整体来看，某些对象没有离群特征，但是从局部来看，却显示了一定的离群性。</td>
</tr>
<tr>
<td>从数据类型</td>
<td>数值型离群点和分类型离群点</td>
<td>这是以数据集的属性类型进行划分的</td>
</tr>
<tr>
<td>从属性个数</td>
<td>一维离群点和多维离群点</td>
<td>一个对象可能有一个或多个属性</td>
</tr>
</table>
### 5.5.1 离群点检测方法
>常用离群点统计方法如下：  

* 基于统计
* 基于邻近度
* 基于密度
* 基于聚类

### 5.5.2 基于模型的离群点检测方法
1.一元正态分布中的离群点检测    
2.混合模型的离群点检测    
### 5.5.3 基于聚类的离群点检测方法
1.丢弃原理其他簇的小簇  
2.基于原型的聚类  

	# 参数初始化
	inputfile = 'consumption_data.xls'  # 销量及其他属性数据
	k = 3  # 聚类的类别
	threshold = 2  # 离散点阈值
	iteration = 500  # 聚类最大循环次数
	data = pd.read_excel(inputfile, index_col='Id')  # 读取数据
	data_zs = 1.0 * (data - data.mean()) / data.std()  # 数据标准化
	
	from sklearn.cluster import KMeans
	
	model = KMeans(n_clusters=k, init='k-means++', n_init=4)  # 分为k类，并发数4
	model.fit(data_zs)  # 开始聚类
	
	# 标准化数据及其类别
	r = pd.concat([data_zs, pd.Series(model.labels_, index=data.index)], axis=1)  # 每个样本对应的类别
	r.columns = list(data.columns) + ['聚类类别']  # 重命名表头
	
	norm = []
	for i in range(k):  # 逐一处理
	    norm_tmp = r[['R', 'F', 'M']][r['聚类类别'] == i] - model.cluster_centers_[i]
	    norm_tmp = norm_tmp.apply(np.linalg.norm, axis=1)  # 求出绝对距离
	    norm.append(norm_tmp / norm_tmp.median())  # 求相对距离并添加
	
	norm = pd.concat(norm)  # 合并
	
	import matplotlib.pyplot as plt
	
	plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
	plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号
	norm[norm <= threshold].plot(style='go')  # 正常点
	
	discrete_points = norm[norm > threshold]  # 离群点
	discrete_points.plot(style='ro')
	
	for i in range(len(discrete_points)):  # 离群点做标记
	    id = discrete_points.index[i]
	    n = discrete_points.iloc[i]
	    plt.annotate('(%s, %0.2f)' % (id, n), xy=(id, n), xytext=(id, n))
	
	plt.xlabel('编号')
	plt.ylabel('相对距离')
	plt.show()

# 第6章 电力窃漏电用户自动识别
	# 电力窃漏电用户自动识别
	import pandas as pd
	from random import shuffle
	
	# 导入数据并打乱数据
	inputfile = 'model.xls'
	data = pd.read_excel(inputfile)
	data = data.as_matrix()
	shuffle(data)
	
	# 将数据分为80%的训练和20%的测试
	num_train = int(len(data) * 0.8)
	train = data[:num_train, :]
	test = data[num_train:, :]
	
	def cm_plot(y, yp):
	    from sklearn.metrics import confusion_matrix  # 导入混淆矩阵函数
	
	    cm = confusion_matrix(y, yp)  # 混淆矩阵
	
	    import matplotlib.pyplot as plt  # 导入作图库
	    plt.matshow(cm, cmap=plt.cm.Greens)  # 画混淆矩阵图，配色风格使用cm.Greens，更多风格请参考官网。
	    plt.colorbar()  # 颜色标签
	
	    for x in range(len(cm)):  # 数据标签
	        for y in range(len(cm)):
	            plt.annotate(cm[x, y], xy=(x, y), horizontalalignment='center', verticalalignment='center')
	
	    plt.ylabel('True label')  # 坐标轴标签
	    plt.xlabel('Predicted label')  # 坐标轴标签
	    return plt
	
	
	# 使用LM神经网络
	'''
	from keras.models import Sequential  # 导入神经网络初始化函数
	from keras.layers.core import Dense, Activation  # 导入神经网路层函数和激活函数
	
	netfile = 'net.model'
	net = Sequential()  # 建立神经网络
	net.add(Dense(input_dim=3, units=10))
	net.add(Activation('relu'))
	net.add(Dense(input_dim=10, units=1))
	net.add(Activation('sigmoid'))
	net.compile(loss='binary_crossentropy', optimizer='adam')  # 编译模型
	net.fit(train[:, :3], train[:, 3], epochs=1000, batch_size=1)  # 训练模型1000次
	net.save_weights(netfile)  # 保存模型
	
	predict_result = net.predict_classes(train[:, :3]).reshape(len(train))  # 预测结果变形
	
	cm_plot(train[:, 3], predict_result).show()
	'''
	# 使用CART决策树
	from sklearn.tree import DecisionTreeClassifier
	
	treefile = 'tree.pkl'
	tree = DecisionTreeClassifier()
	tree.fit(train[:, :3], train[:, 3])
	
	from sklearn.externals import joblib
	joblib.dump(tree, treefile)  # 保存模型
	
	from sklearn.metrics import confusion_matrix #导入混淆矩阵函数
	cm_plot(train[:, 3], tree.predict(train[:, :3])).show()
	
	cm = confusion_matrix(train[:,3], tree.predict(train[:,:3])) #混淆矩阵
	
	import matplotlib.pyplot as plt #导入作图库
	
	from sklearn.metrics import roc_curve #导入ROC曲线函数
	fpr, tpr, thresholds = roc_curve(test[:,3], tree.predict_proba(test[:,:3])[:,1], pos_label=1)
	plt.plot(fpr, tpr, linewidth=2, label = 'ROC of CART', color = 'green') #作出ROC曲线
	plt.xlabel('False Positive Rate') #坐标轴标签
	plt.ylabel('True Positive Rate') #坐标轴标签
	plt.ylim(0,1.05) #边界范围
	plt.xlim(0,1.05) #边界范围
	plt.legend(loc=4) #图例
	plt.show() #显示作图结果

# 第7章 航空公司客户价值分析
	
	import pandas as pd
	
	datafile = 'air_data.csv'
	resultfile = 'explore.xls'
	
	# data = pd.read_csv(datafile, encoding='utf-8')
	# print(type(data))
	# 对数据进行缺失值和异常值分析
	'''
	explore = data.describe(percentiles=[], include='all').T  # percentiles是指计算多少的分位数表，
	
	explore['null'] = len(data) - explore['count']  # 计算获得为空的个数
	explore = explore[['null', 'max', 'min']]
	explore.columns = ['空数值', '最大值', '最小值']
	explore.to_excel(resultfile)
	'''
	
	# 数据清洗
	cleanfile = 'data_clean.xls'
	'''
	data = data[data['SUM_YR_1'].notnull() & data['SUM_YR_2'].notnull()]  # 票价非空才保留
	# 只保留票价非零或平均折扣率与总飞行公里数同时为0
	index1 = data['SUM_YR_1'] != 0
	index2 = data['SUM_YR_2'] != 0
	index3 = (data['SEG_KM_SUM'] == 0) & (data['avg_discount'] == 0) #该规则是“与”
	data = data[index1 | index2 | index3]
	data.to_excel(cleanfile)
	'''
	# 属性规约
	'''
	data = pd.read_excel(cleanfile)
	filename='zscore1data.xls'
	data=data[['LOAD_TIME','FFP_DATE','LAST_TO_END','FLIGHT_COUNT','SEG_KM_SUM','avg_discount']]
	data.to_excel(filename,index=False)
	
	# 数据变换
	
	filename = 'zscore1data.xls'
	data = pd.read_excel(filename)
	
	datazs=pd.DataFrame(columns=['L','R','F','M','C'])
	datazs['L']=data['avg_discount']
	datazs['R']=data['LAST_TO_END']
	datazs['F']=data['FLIGHT_COUNT']
	datazs['M']=data['SEG_KM_SUM']
	datazs['C']=data['avg_discount']
	datazs.sort_values(by='C')
	datazs.to_excel('zscore2data.xls',index=False)
	'''
	# 标准差标准化
	'''
	dataza = pd.read_excel('zscoredata.xls')
	zscoredfile='zscoreddate1.xls'
	dataza=(dataza-dataza.mean(axis=0))/(dataza.std(axis=0))
	dataza.columns=['Z'+i for i in dataza.columns]
	dataza.to_excel(zscoredfile,index=False)
	'''
	
	# K-Means聚类算法
	from sklearn.cluster import KMeans
	
	data = pd.read_excel('zscoreddata.xls')
	kmodel=KMeans(n_clusters=5,init='k-means++',n_init=4)
	kmodel.fit(data)
	print(kmodel.cluster_centers_)
	print(kmodel.labels_)
# 第8章 中医证型关联规则挖掘
	import pandas as pd
	from sklearn.cluster import KMeans
	
	'''
	processedfile = 'data_processed.xls'  # 数据处理后文件
	data = pd.read_excel('data.xls')
	typelabel = {'肝气郁结证型系数': 'A', '热毒蕴结证型系数': 'B', '冲任失调证型系数': 'C', '气血两虚证型系数': 'D', '脾胃虚弱证型系数': 'E', '肝肾阴虚证型系数': 'F'}
	keys = list(typelabel.keys())
	result = pd.DataFrame()
	if __name__ == '__main__':  # 判断是否主窗口运行，如果是将代码保存为.py后运行，则需要这句，如果直接复制到命令窗口运行，则不需要这句。
	    for i in range(len(keys)):
	        # 调用k-means算法，进行聚类离散化
	        print('正在进行“%s”的聚类...' % keys[i])
	        kmodel = KMeans(n_clusters=4)  # n_jobs是并行数，一般等于CPU数较好
	        kmodel.fit(data[[keys[i]]].as_matrix())  # 训练模型
	
	        r1 = pd.DataFrame(kmodel.cluster_centers_, columns=[typelabel[keys[i]]])  # 聚类中心
	        r2 = pd.Series(kmodel.labels_).value_counts()  # 分类统计
	        r2 = pd.DataFrame(r2, columns=[typelabel[keys[i]] + 'n'])  # 转为DataFrame，记录各个类别的数目
	        r = pd.concat([r1, r2], axis=1).sort_values(typelabel[keys[i]])  # 匹配聚类中心和类别数目
	        r.index = [1, 2, 3, 4]
	
	        r[typelabel[keys[i]]] = pd.rolling_mean(r[typelabel[keys[i]]], 2)  # rolling_mean()用来计算相邻2列的均值，以此作为边界点。
	        r[typelabel[keys[i]]][1] = 0.0  # 这两句代码将原来的聚类中心改为边界点。
	        result = result.append(r.T)
	
	    result = result.sort_index(axis=0, ascending=True)  # 以Index排序，即以A,B,C,D,E,F顺序排# 以Index排序，即以A,B,C,D,E,F顺序排
	    result.to_excel(processedfile)
	'''
	import time #导入时间库用来计算用时
	
	# 自定义连接函数，用于实现L_{k-1}到C_k的连接
	def connect_string(x, ms):
	    x = list(map(lambda i: sorted(i.split(ms)), x))
	    l = len(x[0])
	    r = []
	    for i in range(len(x)):
	        for j in range(i, len(x)):
	            if x[i][:l - 1] == x[j][:l - 1] and x[i][l - 1] != x[j][l - 1]:
	                r.append(x[i][:l - 1] + sorted([x[j][l - 1], x[i][l - 1]]))
	    return r
	
	
	# 寻找关联规则的函数
	def find_rule(d, support, confidence, ms='--'):
	    result = pd.DataFrame(index=['support', 'confidence'])  # 定义输出结果
	
	    support_series = 1.0 * d.sum() / len(d)  # 支持度序列
	    column = list(support_series[support_series > support].index)  # 初步根据支持度筛选
	    k = 0
	
	    while len(column) > 1:
	        k = k + 1
	        print('\n正在进行第%s次搜索...' % k)
	        column = connect_string(column, ms)
	        print('数目：%s...' % len(column))
	        sf = lambda i: d[i].prod(axis=1, numeric_only=True)  # 新一批支持度的计算函数
	
	        # 创建连接数据，这一步耗时、耗内存最严重。当数据集较大时，可以考虑并行运算优化。
	        d_2 = pd.DataFrame(list(map(sf, column)), index=[ms.join(i) for i in column]).T
	
	        support_series_2 = 1.0 * d_2[[ms.join(i) for i in column]].sum() / len(d)  # 计算连接后的支持度
	        column = list(support_series_2[support_series_2 > support].index)  # 新一轮支持度筛选
	        support_series = support_series.append(support_series_2)
	        column2 = []
	
	        for i in column:  # 遍历可能的推理，如{A,B,C}究竟是A+B-->C还是B+C-->A还是C+A-->B？
	            i = i.split(ms)
	            for j in range(len(i)):
	                column2.append(i[:j] + i[j + 1:] + i[j:j + 1])
	
	        cofidence_series = pd.Series(index=[ms.join(i) for i in column2])  # 定义置信度序列
	
	        for i in column2:  # 计算置信度序列
	            cofidence_series[ms.join(i)] = support_series[ms.join(sorted(i))] / support_series[ms.join(i[:len(i) - 1])]
	
	        for i in cofidence_series[cofidence_series > confidence].index:  # 置信度筛选
	            result[i] = 0.0
	            result[i]['confidence'] = cofidence_series[i]
	            result[i]['support'] = support_series[ms.join(sorted(i.split(ms)))]
	
	    result = result.T.sort_values(by=['confidence', 'support'], ascending=False)  # 结果整理，输出
	    print('\n结果为：')
	    print(result)
	
	    return result
	
	
	inputfile = 'apriori.txt' #输入事务集文件
	data = pd.read_csv(inputfile, header=None, dtype = object)
	
	start = time.clock() #计时开始
	print('\n转换原始数据至0-1矩阵...')
	ct = lambda x : pd.Series(1, index = x[pd.notnull(x)]) #转换0-1矩阵的过渡函数
	b = map(ct, data.as_matrix()) #用map方式执行
	data = pd.DataFrame(list(b)).fillna(0) #实现矩阵转换，空值用0填充
	end = time.clock() #计时结束
	print('\n转换完毕，用时：%0.2f秒' %(end-start))
	del b #删除中间变量b，节省内存
	
	support = 0.06 #最小支持度
	confidence = 0.75 #最小置信度
	ms = '---' #连接符，默认'--'，用来区分不同元素，如A--B。需要保证原始表格中不含有该字符
	
	start = time.clock() #计时开始
	print('\n开始搜索关联规则...')
	find_rule(data, support, confidence, ms)
	end = time.clock() #计时结束
	print('\n搜索完成，用时：%0.2f秒' %(end-start))

# 第9章 基于水色图像的水质评价
	
	import pandas as pd
	
	inputfile = 'moment.csv' #数据文件
	outputfile1 = 'cm_train.xls' #训练样本混淆矩阵保存路径
	outputfile2 = 'cm_test.xls' #测试样本混淆矩阵保存路径
	data = pd.read_csv(inputfile, encoding = 'gbk') #读取数据，指定编码为gbk
	data = data.as_matrix()
	
	from numpy.random import shuffle #引入随机函数
	shuffle(data) #随机打乱数据
	data_train = data[:int(0.8*len(data)), :] #选取前80%为训练数据
	data_test = data[int(0.8*len(data)):, :] #选取前20%为测试数据
	
	#构造特征和标签
	x_train = data_train[:, 2:]*30
	y_train = data_train[:, 0].astype(int)
	x_test = data_test[:, 2:]*30
	y_test = data_test[:, 0].astype(int)
	
	#导入模型相关的函数，建立并且训练模型
	from sklearn import svm
	model = svm.SVC()
	model.fit(x_train, y_train)
	import pickle
	pickle.dump(model, open('svm.model', 'wb'))
	#最后一句保存模型，以后可以通过下面语句重新加载模型：
	#model = pickle.load(open('../tmp/svm.model', 'rb'))
	
	#导入输出相关的库，生成混淆矩阵
	from sklearn import metrics
	cm_train = metrics.confusion_matrix(y_train, model.predict(x_train)) #训练样本的混淆矩阵
	cm_test = metrics.confusion_matrix(y_test, model.predict(x_test)) #测试样本的混淆矩阵
	
	#保存结果
	pd.DataFrame(cm_train, index = range(1, 6), columns = range(1, 6)).to_excel(outputfile1)
	pd.DataFrame(cm_test, index = range(1, 6), columns = range(1, 6)).to_excel(outputfile2)
# 第10章 家用电器用户行为分析与事件识别
	
	# 用水事件划分
	import pandas as pd
	import numpy as np
	
	threshold = pd.Timedelta('4 min')  # 阈值为分钟
	inputfile = 'water_heater.xls'  # 输入数据路径,需要使用Excel格式
	outputfile = 'dividsequence.xls'  # 输出数据路径,需要使用Excel格式
	
	data = pd.read_excel(inputfile)
	data['发生时间'] = pd.to_datetime(data['发生时间'], format='%Y%m%d%H%M%S')
	data = data[data['水流量'] > 0]  # 只要流量大于0的记录
	d = data['发生时间'].diff() > threshold  # 相邻时间作差分，比较是否大于阈值
	data['事件编号'] = d.cumsum() + 1  # 通过累积求和的方式为事件编号
	
	data.to_excel(outputfile)
	
	inputfile = 'water_heater.xls'  # 输入数据路径,需要使用Excel格式
	n = 4  # 使用以后四个点的平均斜率
	
	threshold = pd.Timedelta(minutes=5)  # 专家阈值
	data = pd.read_excel(inputfile)
	data['发生时间'] = pd.to_datetime(data['发生时间'], format='%Y%m%d%H%M%S')
	data = data[data['水流量'] > 0]  # 只要流量大于0的记录
	
	
	def event_num(ts):
	    d = data['发生时间'].diff() > ts  # 相邻时间作差分，比较是否大于阈值
	    return d.sum() + 1  # 这样直接返回事件数
	
	
	dt = [pd.Timedelta(minutes=i) for i in np.arange(1, 9, 0.25)]
	h = pd.DataFrame(dt, columns=['阈值'])  # 定义阈值列
	h['事件数'] = h['阈值'].apply(event_num)  # 计算每个阈值对应的事件数
	h['斜率'] = h['事件数'].diff() / 0.25  # 计算每两个相邻点对应的斜率
	h['斜率指标'] = pd.Series.rolling(self=h['斜率'], window=n,
	                              center=False, ).mean()  # pd.rolling_mean(h['斜率'].abs(), n)  # 采用后n个的斜率绝对值平均作为斜率指标
	ts = h['阈值'][h['斜率指标'].idxmin() - n]
	# 注：用idxmin返回最小值的Index，由于rolling_mean()自动计算的是前n个斜率的绝对值平均
	# 所以结果要进行平移（-n）
	
	if ts > threshold:
	    ts = pd.Timedelta(minutes=4)
	
	print(ts)
	
	inputfile1 = 'train_neural_network_data.xls'  # 训练数据
	inputfile2 = 'test_neural_network_data.xls'  # 测试数据
	testoutputfile = 'test_output_data.xls'  # 测试数据模型输出文件
	data_train = pd.read_excel(inputfile1)  # 读入训练数据(由日志标记事件是否为洗浴)
	data_test = pd.read_excel(inputfile2)  # 读入测试数据(由日志标记事件是否为洗浴)
	y_train = data_train.iloc[:, 4].as_matrix()  # 训练样本标签列
	x_train = data_train.iloc[:, 5:17].as_matrix()  # 训练样本特征
	y_test = data_test.iloc[:, 4].as_matrix()  # 测试样本标签列
	x_test = data_test.iloc[:, 5:17].as_matrix()  # 测试样本特征
	
	from keras.models import Sequential
	from keras.layers.core import Dense, Dropout, Activation
	
	model = Sequential()  # 建立模型
	model.add(Dense(input_dim=11, units=17))  # 添加输入层、隐藏层的连接
	model.add(Activation('relu'))  # 以Relu函数为激活函数
	model.add(Dense(input_dim=17, units=10))  # 添加隐藏层、隐藏层的连接
	model.add(Activation('relu'))  # 以Relu函数为激活函数
	model.add(Dense(input_dim=10, units=1))  # 添加隐藏层、输出层的连接
	model.add(Activation('sigmoid'))  # 以sigmoid函数为激活函数
	# 编译模型，损失函数为binary_crossentropy，用adam法求解
	model.compile(loss='binary_crossentropy', optimizer='adam')
	
	model.fit(x_train, y_train, epochs=100, batch_size=1)  # 训练模型
	model.save_weights('net.model')  # 保存模型参数
	
	r = pd.DataFrame(model.predict_classes(x_test), columns=['预测结果'])
	pd.concat([data_test.iloc[:, :5], r], axis=1).to_excel(testoutputfile)
	model.predict(x_test)
# 第11章 应用系统负载分析与磁盘容量测试
>平稳性检验：为了确定原始数据序列中没有随机趋势或确定趋势，需要对数据进行平稳性检验，否则将会产生“伪回归”的现象；
	
	# 参数初始化
	discfile = 'discdata_processed.xls'
	predictnum = 5  # 不使用最后5个数据
	
	data = pd.read_excel(discfile)
	data = data.iloc[: len(data) - 5]  # 不检测最后5个数据
	
	# 平稳性检测
	from statsmodels.tsa.stattools import adfuller as ADF
	
	diff = 0
	adf = ADF(data['CWXT_DB:184:D:\\'])
	while adf[1] > 0.05:
	    diff = diff + 1
	    adf = ADF(data['CWXT_DB:184:D:\\'].diff(diff).dropna())
	
	print(u'原始序列经过%s阶差分后归于平稳，p值为%s' % (diff, adf[1]))

>白噪声检验：验证序列中有用的信息是否已被提取完毕。  


	#参数初始化
	discfile = '../data/discdata_processed.xls'
	
	data = pd.read_excel(discfile)
	data = data.iloc[: len(data)-5] #不使用最后5个数据
	
	#白噪声检测
	from statsmodels.stats.diagnostic import acorr_ljungbox
	
	[[lb], [p]] = acorr_ljungbox(data['CWXT_DB:184:D:\\'], lags = 1)
	if p < 0.05:
	  print(u'原始序列为非白噪声序列，对应的p值为：%s' %p)
	else:
	  print(u'原始该序列为白噪声序列，对应的p值为：%s' %p)
	
	[[lb], [p]] = acorr_ljungbox(data['CWXT_DB:184:D:\\'].diff().dropna(), lags = 1)
	if p < 0.05:
	  print(u'一阶差分序列为非白噪声序列，对应的p值为：%s' %p)
	else:
	  print(u'一阶差分该序列为白噪声序列，对应的p值为：%s' %p)

>模型识别：采用极大似然比方法进行模型的参数估计，估计各个参数的值。然后针对各个不同模型，采用BIC信息准则对模型进行定阶，确定p、q参数，从而选择最优模型。
	
	
	#参数初始化
	discfile = '../data/discdata_processed.xls'
	
	data = pd.read_excel(discfile, index_col = 'COLLECTTIME')
	data = data.iloc[: len(data)-5] #不使用最后5个数据
	xdata = data['CWXT_DB:184:D:\\']
	
	from statsmodels.tsa.arima_model import ARIMA
	
	#定阶
	pmax = int(len(xdata)/10) #一般阶数不超过length/10
	qmax = int(len(xdata)/10) #一般阶数不超过length/10
	bic_matrix = [] #bic矩阵
	for p in range(pmax+1):
	  tmp = []
	  for q in range(qmax+1):
	    try: #存在部分报错，所以用try来跳过报错。
	      tmp.append(ARIMA(xdata, (p,1,q)).fit().bic)
	    except:
	      tmp.append(None)
	  bic_matrix.append(tmp)
	
	bic_matrix = pd.DataFrame(bic_matrix) #从中可以找出最小值
	
	p,q = bic_matrix.stack().idxmin() #先用stack展平，然后用idxmin找出最小值位置。
	print(u'BIC最小的p值和q值为：%s、%s' %(p,q))

>模型检验：模型确定后，检验其残差序列是否为白噪声，如果不是白噪声，说明残差中还存在有用的信息，需要修改模型或进一步提取。

>模型预测：通过检验的模型进行预测。

	
	#参数初始化
	discfile = '../data/discdata_processed.xls'
	lagnum = 12 #残差延迟个数
	
	data = pd.read_excel(discfile, index_col = 'COLLECTTIME')
	data = data.iloc[: len(data)-5] #不使用最后5个数据
	xdata = data['CWXT_DB:184:D:\\']
	
	from statsmodels.tsa.arima_model import ARIMA #建立ARIMA(0,1,1)模型
	
	arima = ARIMA(xdata, (0, 1, 1)).fit() #建立并训练模型
	xdata_pred = arima.predict(typ = 'levels') #预测
	pred_error = (xdata_pred - xdata).dropna() #计算残差
	
	from statsmodels.stats.diagnostic import acorr_ljungbox #白噪声检验
	
	lb, p= acorr_ljungbox(pred_error, lags = lagnum)
	h = (p < 0.05).sum() #p值小于0.05，认为是非白噪声。
	if h > 0:
	  print(u'模型ARIMA(0,1,1)不符合白噪声检验')
	else:
	  print(u'模型ARIMA(0,1,1)符合白噪声检验')

>模型评价：标准为评价绝对误差、平方根误差和评分绝对百分误差。

	
	#参数初始化
	file = '../data/predictdata.xls'
	data = pd.read_excel(file)
	
	#计算误差
	abs_ = (data[u'预测值'] - data[u'实际值']).abs()
	mae_ = abs_.mean() # mae
	rmse_ = ((abs_**2).mean())**0.5 # rmse
	mape_ = (abs_/data[u'实际值']).mean() # mape
	
	print(u'平均绝对误差为：%0.4f，\n均方根误差为：%0.4f，\n平均绝对百分误差为：%0.6f。' %(mae_, rmse_, mape_))