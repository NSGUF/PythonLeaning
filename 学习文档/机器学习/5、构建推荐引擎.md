# 构架推荐引擎
## 1、简介
>**推荐引擎**是一个能预测用户兴趣点的模型。通过预测当前用户可能喜欢的内容，将相应的东西从数据库中筛选出来，这样的推荐引擎有助于将用户和数据集中的合适内容连接起来。推荐引擎通常有**协同过滤**（collaborative filtering）或**基于内容的过滤**（content-based filtering），这两种过滤的方法不同之处在于挖掘推荐的方式，协同过滤从当前用户过去的行为和其他用户对当前用户的评价来构建模型，然后该模型预测这个用户可能感兴趣的内容；而基于内容的过滤用商品本身的特征来给用户推荐更多的商品，商品间的相似度是模型主要的关注点。  
## 2、为数据处理构建函数组合
>机器学习系统中的重要组成部分是数据处理流水线，在数据被输入到机器学习算法中进行训练之前，需要对数据做各种各样的处理，使得该数据可以被算法利用。在构建一个准确的、可扩展的机器学习系统的过程中，拥有一个健壮的数据处理流水线非常重要。通常数据处理流水线就是使用基本函数的组合。不推荐使用嵌套或循环的方式调用这些函数，而是用函数式编程的方式构建函数组合。可重用的函数组合例子如下：

	import numpy as np
	from functools import reduce
	# 定义一个函数，函数每个元素加3
	def add3(input_array):
	    return map(lambda x:x+3 ,input_array)
	
	# 每个元素乘以2
	def mul2(input_array):
	    return map(lambda x:x*2 ,input_array)
	
	# 每个元素加5
	def sub5(input_array):
	    return map(lambda x:x+5,input_array)
	
	# 函数组合
	def function_composer(*args):
	    return reduce(lambda f,g:lambda x:f(g(x)),args)
	
	if __name__=='__main__':
	    arr=np.array([1,2,3,4])
	    # 调用,该方式是嵌套不可读的，当再次运用时需要重复写
	    result=function_composer(add3,mul2,sub5)(arr)
	    print(list(result))
## 3、构建机器学习流水线  
>scikit-learn库中包含了构建机器学习流水线的方法，只需指定函数，它就会构建一个组合对象，使数据通过整个流水线。流水线可包括如预处理、特征选择、监督式学习、非监督式学习等函数。下面例子中构建一个流水线，以便输入特征向量、选择最好的k个特征、用随机森林分类器进行分类等。

from sklearn.datasets import samples_generator
from sklearn.feature_selection import SelectKBest,f_regression
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline

	# 1、创建数据
	x,y=samples_generator.make_classification(n_features=20,n_informative=4,n_redundant=0,random_state=5)
	# 2、创建特征选择器
	selection_k_best=SelectKBest(f_regression,k=10)
	# 3、随机森林
	classifier=RandomForestClassifier(n_estimators=50,max_depth=4)
	# 4、创建流水线
	pipline_classifier=Pipeline([('selector',selection_k_best),('rf',classifier)])
	# pipline_classifier.set_params(selector__k=6,rf__n_estimators=25)# 更改上面的k值和n_estimators
	# 5、训练分类器并预测
	pipline_classifier.fit(x,y)
	prediction=pipline_classifier.predict(x)
	# 6、评价分类器性能
	print(pipline_classifier.score(x,y))
	# 7、查看被选中的特征
	feature_status=pipline_classifier.named_steps['selector'].get_support()
	selected_feature=[]
	for count,item in enumerate(feature_status):
	    if item:
	        selected_feature.append(count)
	print('  '.join([str(x) for x in selected_feature]))

## 4、寻找最近邻
>最近邻模型是指一个通用算法类，其目的是根据训练数据集中的最近领数量来做决策。  

	# 寻找最近邻
	import matplotlib.pyplot as plt
	from sklearn.neighbors import NearestNeighbors
	# 初始化数据
	X = np.array([[1, 1], [1, 3], [2, 2], [2.5, 5], [3, 1],
	        [4, 2], [2, 3.5], [3, 3], [3.5, 4]])
	num_neighbors=3
	
	input_point=[2.6,1.7]
	input_point_arr=np.array(input_point).reshape(1,-1)
	# 初始化最邻近对象并训练
	knn=NearestNeighbors(n_neighbors=num_neighbors,algorithm='ball_tree').fit(X)
	# 获取input_point的前num_neighbors距离的距离和在X的索引
	distances,indices=knn.kneighbors(input_point_arr)
	
	for i,index in enumerate(indices[0][:num_neighbors]):
	    print(str(i+1),'-->',X[index])# 打印
	
	# 画图
	plt.figure()
	plt.scatter(X[:,0],X[:,1],marker='o',s=25,color='k')
	plt.scatter(X[indices[0][:]][:,0],X[indices[0][:]][:,1],
	            marker='o',s=150,color='k',facecolors='none')
	plt.scatter(input_point[0],input_point[1],marker='x'
	            ,s=150,color='k',facecolors='none')
	plt.show()
## 5、构建一个KNN分类器
>KNN(k-nearest neighbors)是用k个最近邻的训练数据集来寻找未知对象分类的一种算法。若找未知数据点属于哪个类，可找到KNN并做一个多数表决。[详细介绍](http://www.fon.hum.uva.nl/praat/manual/kNN_classifiers_1__What_is_a_kNN_classifier_.html)     

	from sklearn import neighbors, datasets
	
	# 1、获取数据，分成X和y
	data = load_data('data_nn_classifier.txt')
	X = data[:, :-1]
	y = data[:, -1].astype(np.int)
	# 2、将每个点画出来
	plt.figure()
	markers = '^sov<>hp'
	mapper = np.array([markers[i] for i in y])
	for i in range(len(y)):
	    plt.scatter(X[i, 0], X[i, 1], edgecolors='black', facecolors='none', marker=mapper[i])
	plt.show()
	
	# 3、创建knn分类器并训练
	classifier = neighbors.KNeighborsClassifier(n_neighbors=10, weights='distance')
	classifier.fit(X, y)
	
	# 4、画出边界
	score = 0.01  # 网格的大小
	xmin, xmax = X[:, 0].min() - 1, X[:, 0].max() + 1
	ymin, ymax = X[:, 1].min() - 1, X[:, 1].max() + 1
	
	x_grid, y_grid = np.meshgrid(np.arange(xmin, xmax, score), np.arange(ymin, ymax, score))
	predicted_values = classifier.predict(np.c_[x_grid.ravel(), y_grid.ravel()])
	print(x_grid.shape)
	predicted_values = predicted_values.reshape(x_grid.shape)
	plt.figure()
	plt.pcolormesh(x_grid,y_grid,predicted_values,cmap=cm.Pastel1)
	# 5、画数据
	for i in range(len(y)):
	    plt.scatter(X[i, 0], X[i, 1], edgecolors='black', facecolors='none', marker=mapper[i])
	plt.show()
	# 6、对模型进行测试
	test_datapoint=np.array([[4.5,3.6]])
	plt.figure()
	for i in range(len(y)):
	    plt.scatter(X[i,0],X[i,1],edgecolors='black',facecolors='none',marker=mapper[i])
	plt.scatter(test_datapoint[0,0],test_datapoint[0,1],marker='x',)
	# 7、获取测试数据点附近n_neighbors个点的位置和距离
	dist,indices=classifier.kneighbors(test_datapoint)
	print(dist)
	print(indices)
	# 8、画出结果
	for i in indices:
	    plt.scatter(X[i,0],X[i,1],s=100)
	plt.show()
	print('属于分类：',classifier.predict(test_datapoint)[0])
## 6、KNN回归器  
>目标：预测连续值的输出。  

    import matplotlib.pyplot as plt
    import numpy as np
    from sklearn import neighbors
    
    # 1、生成正态分布数据
    amplitude=10
    num_points=100
    X=amplitude*np.random.rand(num_points,1)-0.5*amplitude # 生成num_points*1的均匀分布，并且让中心为0
    # 2、加入噪声
    y=np.sinc(X).ravel()# 基本正弦函数
    y+=0.2*(0.5-np.random.rand(y.size))
    # 3、数据可视化
    plt.figure()
    plt.scatter(X,y,s=20,c='k',facecolors='none')
    # 4、创建10密度的网格
    x_values=np.linspace(-0.5*amplitude,0.5*amplitude,10*num_points)[:,np.newaxis]
    print(x_values)
    # 5、定义并训练KNN回归器
    knn_regressor=neighbors.KNeighborsRegressor(8,weights='distance')
    y_values=knn_regressor.fit(X,y).predict(x_values)
    # 6、画出结果
    plt.plot(x_values,y_values,c='k',linestyle='--')
    plt.show()
## 7、欧式距离分数  
>为了构建一个推荐引擎，需要定义一个相似度指标，以便找到与数据库中特定用户相似的用户，欧式距离分数就是这样的指标。  

    import json
    import numpy as np
    
    # 计算欧几里得分数函数
    def euclidean_score(dataset,user1,user2):
        if user1 not in dataset:
            raise TypeError('用户：'+user1+'不在数据集中')
        if user2 not in dataset:
            raise TypeError('用户：'+user2+'不在数据集中')
        rated_by_both={}
        for item in dataset[user1]:# 判断用户是否有相同属性
            if item in dataset[user2]:
                rated_by_both[item]=1
        if len(rated_by_both)==0:
            return 0
        # 如果有相同属性则算出属性之间的差的平方和，最后进行归一化
        squared_differences=[]
        for item in dataset[user1]:
            if item in dataset[user2]:
                squared_differences.append(np.square(dataset[user1][item]-dataset[user2][item]))
        return 1/(1+np.sqrt(np.sum(squared_differences)))# 如果评分相似，则差平方会很小，则评分会很高
    
    if __name__=='__main__':
        data_file='movie_ratings.json'
        # 获取数据
        with open(data_file,'r') as f:
            data=json.loads(f.read())
        user1 = 'John Carson'
        user2 = 'Michelle Peterson'
        # 进行计算
        print(euclidean_score(data,user1,user2))
## 8、皮尔逊相关系数  
>该系数常用于推荐引擎  

    import json
    import numpy as np
    
    def pearson_score(dataset,user1,user2):
        if user1 not in dataset:
            raise TypeError('用户：'+user1+'不在数据集中')
        if user2 not in dataset:
            raise TypeError('用户：'+user2+'不在数据集中')
        rated_by_both={}
        for item in dataset[user1]:
            if item in dataset[user2]:
                rated_by_both[item]=1
        num_ratings=len(rated_by_both)
        # 计算相同电影评分的和
        user1_sum=np.sum([dataset[user1][item] for item in rated_by_both])
        user2_sum=np.sum([dataset[user2][item] for item in rated_by_both])
        # 计算相同电影评分的平方和
        user1_squared_sum=np.sum([np.square(dataset[user1][item]) for item in rated_by_both])
        user2_squared_sum=np.sum([np.square(dataset[user2][item]) for item in rated_by_both])
        # 数据集的乘积之和
        product_sum=np.sum([dataset[user1][item]*dataset[user2][item] for item in rated_by_both])
        # 相关元素
        Sxy=product_sum-(user1_sum*user2_sum/num_ratings)
        Sxx=user1_squared_sum-np.square(user1_sum)/num_ratings
        Syy=user2_squared_sum-np.square(user2_sum)/num_ratings
        if Sxx*Syy==0:
            return 0
        return Sxy/np.sqrt(Sxx*Syy)
    
    if __name__=='__main__':
        data_file = 'movie_ratings.json'
        with open(data_file, 'r') as f:
            data = json.loads(f.read())
    
        user1 = 'John Carson'
        user2 = 'Michelle Peterson'
    
        print(pearson_score(data, user1, user2))
        
        
## 9、寻找数据集中的相似用户
>构建腿甲引擎中一个非常重要的任务是寻找相似的用户，若某位用户生成的推荐信息可同时推荐给其他相似用户。   

    # 寻找特定数量与用户相似的用户
    def find_similar_users(dataset,user,num_users):
        if user not in dataset:
            raise TypeError('用户：'+user+'不在数据库中')
        # 计算user与其他所有用户的皮尔逊相关系数
        scores=np.array([[x,pearson_score(dataset,user,x)] for x in dataset if user!=x])
        # 按照系数的降序排列
        scored_sorted_des=np.argsort(scores[:,1])[::-1]
        # 提取num_users最高分
        top_k=scored_sorted_des[0:num_users]
        return scores[top_k]
    
    if __name__=='__main__':
        data_file = 'movie_ratings.json'
    
        with open(data_file, 'r') as f:
            data = json.loads(f.read())
    
        user = 'John Carson'
        similar_users = find_similar_users(data, user, 3)
        for item in similar_users:
            print(item[0], '\t\t', round(float(item[1]), 2))
## 10、电影推荐  

    def generate_recommendations(dataset,user):
        if user not in dataset:
            raise TypeError('用户'+user+'不在数据集中')
        total_scores={}
        similarity_sums={}
        for u in [x for  x in dataset if x!=user]:
            similarity_score=pearson_score(dataset,user,u)
            if similarity_score<=0:
                continue
            for item in [x for x in dataset[u] if x not in dataset[user or dataset[user][x]==0]]:
                total_scores.update({item:dataset[u][item]*similarity_score})
                similarity_sums.update({item:similarity_score})
            if len(total_scores)==0:
                return ['没有相似']
            movie_ranks = np.array([[total / similarity_sums[item], item]
                                    for item, total in total_scores.items()])
            movie_ranks = movie_ranks[np.argsort(movie_ranks[:, 0])[::-1]]
            recommendations = [movie for _, movie in movie_ranks]
            return recommendations
    
    if __name__=='__main__':
        data_file = 'movie_ratings.json'
    
        with open(data_file, 'r') as f:
            data = json.loads(f.read())
    
        user = 'Michael Henry'
        movies = generate_recommendations(data, user)
        for i, movie in enumerate(movies):
            print(str(i+1) + '. ' + movie)
    
        user = 'John Carson'
        print("\nRecommendations for " + user + ":")
        movies = generate_recommendations(data, user)
        for i, movie in enumerate(movies):
            print(str(i+1) + '. ' + movie)