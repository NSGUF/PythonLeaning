# 机器学习
## 机器学习的基本概念
>**机器学习**的研究领域是发明计算机算法，把数据转化为智能行为。  
>机器学习的一个紧密相关学科是**数据挖掘**，它涉及从大型的数据库中产生新的洞察（不要把它和“挖掘数据”向混淆，指的是挑选最合适的数据来支持某个理论的行为）。  
>机器学习侧重执行一个已知的任务，而数据挖掘则侧重寻找有价值的信息。例如你可能会用机器学习方法去教一个机器人开车，而会利用数据挖掘了解那种类型的车安全性更高。  
>机器学习算法实际上是数据挖掘的前期准备，应用机器学习可以完成不涉及数据挖掘的任务，但是应用数据挖掘方法一定会用到机器学习。  
>机器学习的核心主要侧重与找出复杂数据的意义。  
>机器学习的定义：如果机器学习能够获取经验并且能利用他们在以后类似经验中能够提高他的表现，该机器成为机器学习。  
学习过程：  
1.数据的输入：利用观察、记忆存储以及回忆来提供进一步推理的事实依据。  
2.抽象化：把数据转换成更宽泛的表现形式（broader representation）。  
3.一般化：应用抽象的数据来形成行动的基础。  
**训练：**用一个特定的模型来拟合数据集的过程。  
偏差：算法的结论是系统性的不精确。在任何机器学习任务的抽象化和一般化着两个过程相关联的不可避免的谬误。  
**将机器学习应用于数据的步骤：**  
1、收集数据  
2、探索数据和准备数据  
3、基于数据训练模型  
4、评价模型的性能  
5、改进模型的性能  
**输入的数据：**所有机器学习算法都要求输入训练数据，且数据是以案例（example）和特征（feature）组成的表格形式。  
**案例：**是一个被学习概念的示例性实例，是要分析事物的最基本单位的一组数据。  
**特征：**案例的一个属性或特性。  
**矩阵格式：**是目前机器学习中最常用的数据格式，每一行为一个案例，每一列为一个特征，所以每个案例都有相同数量的特征。  
**机器学习算法分为两类**：用来建立预测模型的有监督学习算法和用来建立描述模型的无监督学习算法。  
**预测模型：**利用数据集中的其他数值来预测另一个值。常用回归。  
**描述性模型：**通过新而有趣的方式总结数据并获得洞察，从而学习任务从这些洞察中收益。常用聚类。
## 懒惰学习--近邻分类（k-Nearest Neighbors，k邻近）
>**概念：**把未标记的案例归类为与他们最相似的带有标记的案例所在的类。  
>**应用领域**1、计算机视觉；2、推荐；3、识别基因数据模型。  
>**优点：**1、简单有效；2、对数据的分布没有要求；3、训练阶段很快    
>**缺点：**  
>1、不产生模型，在发现特征之前关系上的能力有限。  
>2、分类阶段和慢   
>3、需要大量内存  
>4、名义变量（特征）和缺失数据需要额外的处理  
**过程：**  kNN算法将特征处理为一个多维特征空间（feature space）内的坐标。  
1、计算距离；距离越短就越相近；常用欧式距离（Euclidean distance）、曼哈顿距离（Manhattan distance），即计算未标记的案例与他的邻居的距离。  
2、选择一个合适的k；过度拟合和低度拟合训练数据之间的平衡问题成为**偏差-方差权衡**（bias-variance tradeoff).若k非常大，则容易过拟合，即每个案例会属于一个类；反之则低度拟合，例如将水果和蔬菜放入同一类中。通常k为3-10、训练集中案例数量的平方根、基于各种测试数据来测试多个k。  
3、准备kNN算法使用的数据；如果我们需要将每个维度都控制到同样的范围内，可使用min-max标准化（min-max normalization）使所有的值都若在0-1内；公示为X=(X-min(X))/(max(X)-min(X))，意思为在原最小值和原始最大值范围内，原始值到原始最小值的距离有多远。另一种常见变换方法是z-score标准化(z-score standardization)，公式为：X=（X-Mean（X））/StdDev（X），无预定义最大小值；此外可能出现不能够定量处理的变量量化，不能够定量处理的变量量化，所以这里可以用哑变量（dummy coding）其中1表示一个类别，0表示其他。  
**kNN为何是懒惰学习？**在技术上，跳过了抽象化和一般化过程，不会建立一个模型，只是一字不差的存储训练数据，所以该方法被归类为非参数(non-parametric)学习方法，又称为基于实例的学习（instance-based leanring）或机械学习（rote learing）。  
**用kNN算法诊断乳腺癌**  
1、收集数据：从<http://archive.ics.uci.edu/ml> 
# 概率学习-朴素贝叶斯分类  
>**贝叶斯方法的概念：**一个事件的似然估计应建立在手中已有的证据的基础上，事件（event）就是可能的结果，试验（trial）是事件发生一次的机会，如某天的天气、抛掷一枚硬币。  
>1、概率：一个事件的概率通过观测到的数据估计，如抛掷两次硬币，一次正一次反，所以概率为50%。  
>2、联合概率：多个事件的联合发生概率。  
>3、基于贝叶斯的条件概率：即条件概率；公式：后验概率=（似然概率*先验概率）/编辑似然概率；  
**朴素贝叶斯算法：**依据概率原则进行分类的算法，应用贝叶斯定理进行分类的一个简单应用。  
**优点：**1、简单快速有效；2、能处理好噪声数据和缺失的数据；3、需要用来训练好的例子相对较少，但同样能处理好大量的例子；4、很容易获得一个预测的估计概率值。  
**缺点：**1、依赖于一个常用的错误假设，即一样的重要性和独立特征；2、应用在含有大量数值特征的数据集时并不理想；3、概率的估计值相对于预测的类而言更加不可靠。  












































 